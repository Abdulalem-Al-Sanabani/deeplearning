{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BridgingAI Logo](../bridgingai_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Exercise 1: Introduction to PyTorch\n",
    "---\n",
    "1. [PyTorch Basics](#basics)\n",
    "2. [Modules and Optimizers](#training)\n",
    "3. [Data Loading](#data)\n",
    "3. [Digit Classification on MNIST](#mnist)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# many useful functions live directly in the torch module\n",
    "import torch\n",
    "# torch.nn contains network layers, loss functions, etc. as Modules which manage some internal state (eg parameters and buffers)\n",
    "import torch.nn as nn\n",
    "# all of them are also available in functional form (no internal state) in torch.nn.functional\n",
    "import torch.nn.functional as F\n",
    "# torch.optim contains optimizers such as SGD, Adam, etc.\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from exercise_utils.core import make_summary_writer\n",
    "import utils\n",
    "\n",
    "torch.manual_seed(42)  # set seed for reproducibility\n",
    "\n",
    "TWO_LAYER_NET_PATH = \"TwoLayerNet.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"basics\"></a>\n",
    "## 1. PyTorch Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PyTorch](https://pytorch.org/) is a popular deep learning framework. It provides NumPy-like vectorized operations with support for automatic differentiation and hardware acceleration. From a very simple point of view,  \n",
    "\n",
    "`PyTorch = NumPy + Automatic differentiation + GPU support.`\n",
    "\n",
    "Additionally, there is a vast ecosystem of additional packages and libraries based on PyTorch.\n",
    "\n",
    "We will briefly illustrate the basics in the following code cells. For a more comprehensive reference of PyTorch, you can then check the [documentation](https://pytorch.org/docs/stable/) and also the [tutorials](https://pytorch.org/tutorials)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "`torch.Tensor` is PyTorch's fundamental data structure - a generalization of arrays and matrices. Tensors support arbitrary dimensions (from 0D scalars to high-dimensional arrays), different underlying datatypes (such as int8 or fp64), and operations on them can seamlessly utilize different hardware (e.g., CPU or GPU).\n",
    "\n",
    "The following code cells illustrate the basic operations on tensors. They look very similar to numpy operations, and they obey the same shape [broadcasting rules](https://pytorch.org/docs/stable/notes/broadcasting.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Tensors and basic operations\n",
    "\n",
    "# make new tensor by copying existing data\n",
    "a = torch.tensor([1.0, 2.0, 3.0])\n",
    "b = torch.tensor(np.zeros((3, 3)))\n",
    "# shapes must match, but pytorch is smart about broadcasting (like numpy)\n",
    "c = a + b\n",
    "print(a.shape, b.shape, c.shape)\n",
    "print(c)\n",
    "print()\n",
    "\n",
    "# as_tensor tries to reuse the same memory if possible\n",
    "d = torch.as_tensor(4)\n",
    "e = torch.as_tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "# indexing is similar to numpy\n",
    "f = 3 * b[1:, :2] + d * e\n",
    "print(f.shape)\n",
    "print(f)\n",
    "print()\n",
    "\n",
    "# torch provides many functions for creating tensors\n",
    "g = torch.randn((3, 3))  # Random numbers from a normal distribution\n",
    "h = torch.ones_like(g)  # Ones with the same shape as tensor3\n",
    "print(d * e)\n",
    "print()\n",
    "\n",
    "# Matrix Multiplication\n",
    "matrix1 = torch.randn(300, 500)\n",
    "matrix2 = torch.randn(500, 800)\n",
    "matrix_mul = torch.matmul(matrix1, matrix2)\n",
    "matrix_mul2 = matrix1 @ matrix2  # shorthand for matmul\n",
    "\n",
    "print(matrix_mul2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch supports many of NumPy's operations as well. If you want to interface with other libraries that expect NumPy arrays instead of tensors, you can convert a `torch.tensor` to a `numpy.array` by simply calling `.numpy()` on it.\n",
    "\n",
    "Run the following cell to visualize a 3D gaussian distribution - note how we convert the tensors to arrays as we pass them to matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid of points\n",
    "x = torch.linspace(-5, 5, 100)\n",
    "y = torch.linspace(-5, 5, 100)\n",
    "X, Y = torch.meshgrid(x, y, indexing=\"ij\")\n",
    "\n",
    "# Compute the Gaussian function\n",
    "Z = torch.exp(-(X**2 + Y**2) / 2) / (2 * torch.pi)\n",
    "\n",
    "# Create a 3D plot\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "# Plot the surface\n",
    "surf = ax.plot_surface(X.numpy(), Y.numpy(), Z.numpy(), cmap=\"viridis\")\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "ax.set_zlabel(\"Z\")\n",
    "ax.set_title(\"3D Gaussian density function\")\n",
    "fig.colorbar(surf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic differentiation\n",
    "PyTorch's autograd engine ([torch.autoGrad](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)) allows us to compute gradients of the parameters with respect to the loss function. When we perform operations on tensors, PyTorch keeps track of the operations by creating a computation graph under the hood. This computation graph is then used to compute the gradients using Backpropagation.\n",
    "\n",
    "Run the following code cell for a very simple example of autograd in action. What happens when you set `requires_grad=False` for one of the tensors, and what if you multiply one of them with zeros?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set requires_grad=True to track computations\n",
    "a = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "b = torch.tensor([4.0, 5.0, 6.0], requires_grad=True)\n",
    "c = (a + b).sum()\n",
    "\n",
    "# Call backward on a scalar to compute gradients\n",
    "c.backward()\n",
    "\n",
    "print(\"Output:\", c)\n",
    "print(\"Grad a:\", a.grad)\n",
    "print(\"Grad b:\", b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we want to avoid the overhead of tracking computations, or we do not want to compute gradients for some operations (for example during evaluation, or while updating parameters manually). In that case, we can disable gradient tracking using `torch.no_grad()`, which works both as a decorator and a context manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "print(\"With gradient tracking:\", 2 * a)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"Without gradient tracking:\", 2 * a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around with the following code cell to see how we can implement logistic regression with PyTorch's AutoGrad engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "def run_logistic_regression(data, labels, num_iterations=5000, learning_rate=0.1):\n",
    "    weights = torch.randn(data.shape[1], requires_grad=True)\n",
    "\n",
    "    pbar = tqdm(range(num_iterations))\n",
    "    for i in pbar:\n",
    "        # compute the cross entropy loss\n",
    "        logits = data @ weights\n",
    "        pred = torch.sigmoid(logits)\n",
    "        loss = F.binary_cross_entropy(pred, labels)\n",
    "\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # manual implementation of gradient descent\n",
    "        # later, this will be handled by an optimizer from torch.optim\n",
    "        with torch.no_grad():\n",
    "            weights += -learning_rate * weights.grad\n",
    "            # reset gradients (otherwise they accumulate)\n",
    "            weights.grad = None\n",
    "\n",
    "        pbar.set_description(f\"loss: {loss.item():.5f}\")\n",
    "\n",
    "    # return a copy without gradient tracking\n",
    "    return weights.detach()\n",
    "\n",
    "\n",
    "# datasets available: \"binary_blob\", \"two_moons\", \"xor\", \"concentric_circles\"\n",
    "data, labels = utils.get_dataset(\"binary_blob\")\n",
    "\n",
    "\n",
    "def polynomial_features(data, degree):\n",
    "    x1, x2 = data[:, 1], data[:, 2]\n",
    "    features = [data[:, 0]]  # Keep bias column\n",
    "\n",
    "    for i in range(1, degree + 1):\n",
    "        for j in range(i + 1):\n",
    "            features.append((x1 ** (i - j)) * (x2**j))\n",
    "\n",
    "    return torch.stack(features, dim=1)\n",
    "\n",
    "\n",
    "# increase the degree to get a non-linear decision boundary\n",
    "feature_transform = lambda x: polynomial_features(x, degree=1)\n",
    "\n",
    "# Training the model\n",
    "weights = run_logistic_regression(feature_transform(data), labels)\n",
    "\n",
    "# Plot the decision boundary\n",
    "utils.plot_logistic_regression(data, labels, weights, feature_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hardware acceleration\n",
    "PyTorch operations seamlessly run on various hardware:\n",
    "- CPU: Default execution device\n",
    "- CUDA: NVIDIA GPU acceleration (also supports AMD GPUs to some degree)\n",
    "- MPS: Apple Silicon optimization\n",
    "- TPU: Google's Tensor Processing Units\n",
    "\n",
    "In PyTorch, these accelerators are called \"devices\". For example, you can run the operations from above on the GPU, by simply moving the tensors to the correct device. This way, almost all hardware-specific code is abstracted away.\n",
    "\n",
    "You can call the `to()` method on many PyTorch objects to move the associated data to another device, for example `t.to('cuda')` to move a tensor `t` to the default GPU.\n",
    "If you create new tensors, it is often more performant to create them directly on the device - simply specify the the `device` attribute in the constructor.\n",
    "\n",
    "Hardware acceleration and parallelization enables tremendous speedups - you will see that by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this timer takes care of proper device synchronization and warm-up\n",
    "from torch.utils.benchmark import Timer\n",
    "\n",
    "N = 1000\n",
    "\n",
    "\n",
    "def benchmark_matmul(device, nthreads):\n",
    "    a = torch.randn(N, N, device=device)\n",
    "    b = torch.randn(N, N, device=device)\n",
    "\n",
    "    def func():\n",
    "        return a @ b\n",
    "\n",
    "    timer = Timer(\n",
    "        stmt=\"func()\",\n",
    "        globals={\"func\": func},\n",
    "        num_threads=nthreads,\n",
    "        label=f\"matmul ({device})\",\n",
    "    )\n",
    "    time = timer.blocked_autorange().mean\n",
    "\n",
    "    print(f\"{device} ({nthreads} threads): {time*1000:.3f}ms\")\n",
    "\n",
    "\n",
    "nthreads = torch.get_num_threads()\n",
    "# baseline: cpu, using a single core\n",
    "benchmark_matmul(\"cpu\", nthreads=1)\n",
    "# torch uses all available cores by default\n",
    "benchmark_matmul(\"cpu\", nthreads=nthreads)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # dedicated GPU (NVIDIA or AMD)\n",
    "    benchmark_matmul(\"cuda\", nthreads=nthreads)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    # Metal Performance Shaders (MPS) on Apple Silicon\n",
    "    benchmark_matmul(\"mps\", nthreads=nthreads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"training\"></a>\n",
    "## 2. Modules and Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the examples above, we implemented gradient descent from scratch and also handled all the parameters ourselves. Fortunately, PyTorch provides an easier way to do all that, using _modules_ and _optimizers_.\n",
    "\n",
    "`torch.nn.Module` is the base class of most neural network layers in PyTorch. To implement a custom module, you extend this class and implement the `forward` function. The Module instance wraps all relevant state, such as learnable or non-learnable parameters, configuration, or meta information. Learnable parameters are usually tensors wrapped in `torch.nn.Parameter`, which automatically register the data to the enclosing module to make data handling easier.\n",
    "\n",
    "`torch.nn` contains many often-used building blocks of neural networks: linear layers, convolutions, recurrent layers, activation functions, and many more. `torch.nn.functional` provides a functional interface to these functions.\n",
    "\n",
    "`torch.optim` implements various optimizers, learning rate schedulers, and facilities for weight averaging.\n",
    "\n",
    "We implemented a simple custom module in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        # make sure to call the parent constructor\n",
    "        super().__init__()\n",
    "\n",
    "        # nn.Parameter is a special kind of Tensor used for learnable parameters\n",
    "        # these will be automatically added to the list of module parameters\n",
    "        self.W = nn.Parameter(torch.randn(in_features, out_features))\n",
    "        self.b = nn.Parameter(torch.zeros(out_features))\n",
    "\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation(x @ self.W + self.b).sum()\n",
    "\n",
    "\n",
    "mymodule = MyModule(3, 4)\n",
    "for n, p in mymodule.named_parameters():\n",
    "    print(n, p.shape)\n",
    "\n",
    "# to apply a module, simply call it like a function\n",
    "data = torch.randn(2, 3)\n",
    "output = mymodule(data)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, torch already contains a large variety of common operations. To build an MLP, for example, we can use the `torch.nn.Sequential` class, as in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = nn.Sequential(\n",
    "    nn.Linear(3, 4),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(4, 2),\n",
    ")\n",
    "\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we can implement networks and automatically compute gradients using PyTorch. To train networks, we now need to use the gradients to update the parameters, for example, using stochastic gradient descent. PyTorch has us covered, with `torch.optim.SGD` (and several other, more complex optimizers).\n",
    "\n",
    "To use an optimizer, we pass a the network parameters to the constructor, together with the necessary hyperparameters (e.g., the learning rate for SGD). In each iteration, we call `.step()` and `.zero_grad()` to update the parameters and clear the gradients. See how we can easily learn to classify the toy data from above.\n",
    "\n",
    "Play around with this example - what do you observe when you change the network architecture, e.g., a larger or smaller hidden dimension, a different activation function, or more layers? What happens when you omit the call to `.zero_grad()`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 2000\n",
    "hidden_dim = 128\n",
    "# we specify bias=False because the data already has a bias column\n",
    "mlp = nn.Sequential(\n",
    "    nn.Linear(3, hidden_dim, bias=False), nn.Tanh(), nn.Linear(hidden_dim, 1)\n",
    ")\n",
    "optimizer = optim.SGD(mlp.parameters(), lr=0.1)\n",
    "\n",
    "# same datasets as before - \"binary_blob\", \"two_moons\", \"xor\", \"concentric_circles\"\n",
    "data, labels = utils.get_dataset(\"binary_blob\")\n",
    "\n",
    "for _ in tqdm(range(num_iterations)):\n",
    "    # squeeze removes unit dimensions (in this case, the last dimension)\n",
    "    pred = mlp(data).squeeze()\n",
    "    # combines sigmoid and binary cross entropy loss; more numerically stable\n",
    "    loss = F.binary_cross_entropy_with_logits(pred, labels)\n",
    "    loss.backward()\n",
    "\n",
    "    # update the weights\n",
    "    optimizer.step()\n",
    "    # reset the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "# slice the sequential model to simulate a linear model with learned features\n",
    "weights = mlp[-1].weight.detach().T\n",
    "feature_transform = lambda x: mlp[:-1](x)\n",
    "\n",
    "# Plot the decision boundary\n",
    "utils.plot_logistic_regression(data, labels, weights, feature_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving & Loading Checkpoints\n",
    "\n",
    "When training models, it's crucial to save checkpoints periodically. This allows you to resume training from a specific point if it gets interrupted, and to use the trained model for inference later. PyTorch provides an easy way to save and load model internal state via the `.state_dict()` method, which is supported by modules and optimizers.\n",
    "\n",
    "To save a checkpoint, you typically want to include:\n",
    "1. The model's state dictionary\n",
    "2. The optimizer's state dictionary\n",
    "3. Any other relevant information (e.g., epoch number, best validation loss)\n",
    "\n",
    "For example, we might save a checkpoint like this:\n",
    "\n",
    "```python\n",
    "def save_checkpoint(model, optimizer, epoch, best_val_loss, filepath):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_val_loss': best_val_loss,\n",
    "    }, filepath)\n",
    "\n",
    "# Usage example\n",
    "save_checkpoint(model, optimizer, epoch, best_val_loss, 'checkpoint.pth')\n",
    "```\n",
    "\n",
    "And later load a checkpoint to resume training or use the model for inference like this:\n",
    "\n",
    "```python\n",
    "def load_checkpoint(filepath, model, optimizer=None):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "    return model, optimizer, epoch, best_val_loss\n",
    "\n",
    "# Usage example\n",
    "model, optimizer, start_epoch, best_val_loss = load_checkpoint('checkpoint.pth', model, optimizer)\n",
    "```\n",
    "\n",
    "Note that you will encounter an error if the names and shapes of parameters in the checkpoint and in the model definition do not match.\n",
    "\n",
    "We have saved a checkpoint of a two-layer MLP trained on MNIST in this directory. You can load it by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_layer_mlp = nn.Sequential(\n",
    "    nn.Linear(784, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10),\n",
    ")\n",
    "\n",
    "checkpoint = torch.load(TWO_LAYER_NET_PATH, weights_only=True)\n",
    "two_layer_mlp.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>\n",
    "## 3. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training on large datasets, it is not feasible to load the entire dataset into memory at once. Instead, we need to divide the dataset into smaller batches and load them one by one. PyTorch provides the `DataLoader` class for this task: It provides functionality for multi-process dataloading, shuffling, and batching. We only need to implement a `Dataset`, which we can then pass to the dataloader.\n",
    "\n",
    "In many cases, a dataset just maps some index (e.g., an integer) to a sample. You can find good examples for how to create custom datasets in the [documentation](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n",
    "\n",
    "The following cell demonstrates how to use the `DataLoader` class to load the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database). Run the cell to see a few samples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define the transformation to apply to the data\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),  # Convert the image to a tensor\n",
    "        transforms.Normalize((0.5,), (0.5,)),  # Normalize the image data\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Load the MNIST training dataset\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"../datasets\", train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Load the MNIST test dataset\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"../datasets\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "print(len(train_dataset), \"training samples\", len(test_dataset), \"test samples\")\n",
    "\n",
    "# Using multiple workers usually speeds up the data loading (up to a point)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True, num_workers=4\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# To use the data loader, we can just iterate over it...\n",
    "# Let's see what we get from this data loader\n",
    "for images, labels in train_loader:\n",
    "    print(f\"Inside a batch: \")\n",
    "    print(f\"  Image batch shape: {images.shape}\")\n",
    "    print(f\"  Label batch shape: {labels.shape}\")\n",
    "    utils.make_grid(images[:4], labels=labels[:4])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"mnist\"></a>\n",
    "## 4. Digit Classification on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn! You will implement a neural network for MNIST classification using PyTorch. The neural network will be a multi-layer perceptron (MLP) with ReLU activations, defined as:\n",
    "```python\n",
    "# INPUT -> [Linear() -> ReLU()] x num_hidden_layers -> Linear() -> OUTPUT\n",
    "```\n",
    "The loss function should be the softmax cross-entropy loss. \n",
    "\n",
    "**TODOs:**\n",
    "1. Complete `__init__`, `forward` and `optimizer` methods of the `MLP` class.\n",
    "2. Complete the training loop in the `train` function to perform the forward pass, backward pass and update steps.\n",
    "3. Launch tensorboard by running the command `tensorboard --logdir=runs` in the terminal to inspect the training process.\n",
    "\n",
    "<details>\n",
    "<summary>View the training logs with tensorboard</summary>\n",
    "TensorBoard is a convenient tool for visualizing training progress and performance metrics. We will often use tensorboard to log the training process in the following exercises.\n",
    "\n",
    "Follow these steps to get started with TensorBoard:\n",
    "1. Open a terminal.\n",
    "2. Navigate to the directory where your log files are stored (here, this will be the same directory as this notebook).\n",
    "3. Run the following command to launch TensorBoard:\n",
    "```bash\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "4. Once TensorBoard is running, it will provide a URL (the default is http://localhost:6006/) that you can open in your web browser to view the dashboard. If you don't see any data, try refreshing the page and make sure that the log directory is correct.\n",
    "\n",
    "To use tensorboard in your own experiments, have a look at [How to use TensorBoard with PyTorch](https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html).\n",
    "</details>\n",
    "\n",
    "**Hints:**\n",
    "- To construct a neural network, you can use `nn.Sequential` to stack layers sequentially. An alternative way is to use the `nn.ModuleList` class to store the layers in a list, then iterate through the list to perform the forward pass. Please read the documentations to learn how to use them: [Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html), [ModuleList](https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html)\n",
    "- As a sanity check, you can print the test loss for an untrained model. It should be around `2.3` (since the model is initialized randomly, the output is close to uniform distribution, and the cross-entropy loss for uniform distribution is `-log(1/n_classes) = -log(1/10) = 2.3`). If the initial loss is not around `2.3`, there might be a bug in your code.\n",
    "- You can use the cross-entropy loss implemented by PyTorch, please refer this [documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "- Documentation for the `SGD` optimizer can be found [here](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        input_dim = config[\n",
    "            \"input_dim\"\n",
    "        ]  # The number of input features, which will be 28*28=784 for MNIST\n",
    "        hidden_dim = config[\n",
    "            \"hidden_dim\"\n",
    "        ]  # The number of hidden units in each hidden layer\n",
    "        num_hidden_layers = config[\"num_hidden_layers\"]  # The number of hidden layers\n",
    "        output_dim = config[\"output_dim\"]  # The number of output units\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - x: A tensor of shape (batch_size, input_dim)\n",
    "\n",
    "        Returns:\n",
    "        - logits: Model output, a tensor of shape (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return logits\n",
    "\n",
    "\n",
    "def compute_accuracy(model, data_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for img, label in data_loader:\n",
    "            img = img.view(-1, 28 * 28)\n",
    "            logits = model(img)\n",
    "            loss += criterion(logits, label).item()\n",
    "            predicted = logits.argmax(dim=-1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "    return correct / total, loss / len(data_loader)\n",
    "\n",
    "\n",
    "def train(model, train_loader, test_loader, logger, config):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=config[\"lr\"])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    pbar = tqdm(range(config[\"epochs\"]))\n",
    "    for epoch in pbar:\n",
    "        train_losses = []\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for img, label in train_loader:\n",
    "            # \"img\" is the input image data having shape (batch_size, 1, 28, 28)\n",
    "            # \"label\" is the ground truth label having shape (batch_size)\n",
    "            img = img.view(\n",
    "                -1, config[\"input_dim\"]\n",
    "            )  # Flatten the input data for our MLP\n",
    "\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            predicted = logits.argmax(dim=-1)\n",
    "            train_total += label.size(0)\n",
    "            train_correct += (predicted == label).sum().item()\n",
    "\n",
    "        # run validation after each epoch\n",
    "        train_loss = sum(train_losses) / len(train_losses)\n",
    "        train_accuracy = train_correct / train_total\n",
    "        test_accuracy, test_loss = compute_accuracy(model, test_loader)\n",
    "        pbar.set_description(f\"Validation accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "        # log train/test accuracy and train/test loss\n",
    "        logger.add_scalars(\n",
    "            \"Accuracy\", {\"train\": train_accuracy, \"test\": test_accuracy}, epoch\n",
    "        )\n",
    "        logger.add_scalars(\"Loss\", {\"train\": train_loss, \"test\": test_loss}, epoch)\n",
    "\n",
    "        # Sample images from test set and compute predictions\n",
    "        sample_size = 16  # 4x4 grid\n",
    "        sample_images, sample_labels = next(iter(test_loader))\n",
    "        sample_images = sample_images[:sample_size]\n",
    "        sample_labels = sample_labels[:sample_size]\n",
    "        with torch.no_grad():\n",
    "            sample_logits = model(sample_images.view(-1, config[\"input_dim\"]))\n",
    "            _, sample_predictions = torch.max(sample_logits, 1)\n",
    "\n",
    "        # Log some sample predictions\n",
    "        fig_pred = utils.make_grid(sample_images, sample_predictions, sample_labels)\n",
    "        logger.add_figure(\"Sample_Predictions\", fig_pred, epoch)\n",
    "\n",
    "        logger.flush()\n",
    "\n",
    "    return train_accuracy, test_accuracy, fig_pred\n",
    "\n",
    "\n",
    "logger = make_summary_writer(\"MNIST\", \"MLP\")\n",
    "config = {\n",
    "    \"input_dim\": 28 * 28,\n",
    "    \"hidden_dim\": 64,\n",
    "    \"num_hidden_layers\": 1,\n",
    "    \"output_dim\": 10,\n",
    "    \"lr\": 3e-2,\n",
    "    \"epochs\": 10,\n",
    "}\n",
    "\n",
    "model = MLP(config)\n",
    "test_accuracy, test_loss = compute_accuracy(model, test_loader)\n",
    "print(\"Untrained test loss: \", test_loss)\n",
    "print(\"Untrained test accuracy: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If implemented correctly, the default config should yield a test accuracy of around 0.95 after training, which should finish in about a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy, test_accuracy, fig_pred = train(\n",
    "    model, train_loader, test_loader, logger, config\n",
    ")\n",
    "\n",
    "print(\"Finished Training\")\n",
    "print(\"Final train accuracy: \", train_accuracy)\n",
    "print(\"Final test accuracy: \", test_accuracy)\n",
    "fig_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
