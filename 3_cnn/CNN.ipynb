{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BridgingAI Logo](../bridgingai_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Exercise 3: Convolutional Neural Networks and Semantic Segmentation\n",
    "\n",
    "---\n",
    "1. [Convolutional Neural Networks](#chp1-fundamentals)\n",
    "<br/> &#9; 1.1 [Residual Blocks](#chp1-1-residual-block)\n",
    "<br/> &#9; 1.2 [ResNet34](#chp1-2-resnet34)\n",
    "\n",
    "2. [Semantic Segmentation](#chp2-segmentation)\n",
    "<br/> &#9; 2.1 [Dataset Preparation and Visualization](#chp2-1-dataset)\n",
    "<br/> &#9; 2.2 [Metrics and Loss Functions](#chp2-2-metrics-loss)\n",
    "\n",
    "3. [Semantic Segmentation Models](#chp3-segmentation-models)\n",
    "<br/> &#9; 3.1 [Segmentation Model Architecture](#chp3-1-segmentation-model)\n",
    "<br/> &#9; 3.2 [Lite R-ASPP Classifier](#chp3-2-lite-raspp)\n",
    "<br/> &#9; 3.3 [Atrous Convolution](#chp3-3-atrous-convolution)\n",
    "<br/> &#9; 3.4 [Atrous Spatial Pyramid Pooling (ASPP)](#chp3-4-aspp)\n",
    "<br/> &#9; 3.5 [DeepLabV3 Classifier](#chp3-5-deeplabv3)\n",
    "<br/> &#9; 3.6 [DeepLabV3 with with ResNet50 Backbone (Optional)](#chp3-6-deeplabv3-resnet50)\n",
    "\n",
    "4. [Appendix: Performance Benchmarks](#appendix)\n",
    "\n",
    "5. [References](#references)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks (CNNs) are a class of neural networks commonly used for image processing tasks, such as image classification, object detection, or image compression. This exercise will guide you through implementing and experimenting with CNNs, focusing on ResNet architecture and semantic segmentation tasks.\n",
    "\n",
    "### Exercise Overview:\n",
    "\n",
    "Part 1: Implement Residual Networks\n",
    "\n",
    "Part 2: Semantic Segmentation (PASCAL VOC 2012)\n",
    "- Prepare and visualize the dataset\n",
    "- Implement the evaluation metrics and loss function\n",
    "- Compare different architectures for segmentation (Lite R-ASPP, DeepLabV3)\n",
    "- Experiment with different backbones (MobileNetV3, ResNet50)\n",
    "\n",
    "### Compute Requirements\n",
    "The baseline configuration (Lite R-ASPP, 225px resolution, 3000 steps) trains in ~30 minutes on CPU. For more extensive experiments, GPU access is recommended.\n",
    "\n",
    "Performance can be adjusted by:\n",
    "\n",
    "Reducing computational load: Lower resolution, smaller architectures, fewer validation steps\n",
    "Enhancing accuracy (with GPU): 513px resolution, ResNet50/101 backbone, backbone fine-tuning, extended training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "import tests\n",
    "from visualization import visualize_segmentation\n",
    "from config import ExperimentConfig\n",
    "from training import create_dataloaders, CNNTrainer as Trainer\n",
    "from utils import print_model_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"chp1-fundamentals\"></a>\n",
    "# 1. Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will implement Residual Networks (ResNets), a very popular network architecture that was originally introduced for image classification. ResNets facilitate the training of very deep networks by introducing _skip connections_ for improved gradient flow.\n",
    "\n",
    "<a id=\"chp1-1-residual-block\"></a>\n",
    "## 1.1 ResidualBlock Implementation\n",
    "\n",
    "Your first task is to complete the `ResidualBlock` class in the cell below.\n",
    "\n",
    "**TODO 1**: Implement the `forward` method in the `ResidualBlock` class.\n",
    "\n",
    "1. Use the `self.conv1`, `self.bn1`, `self.relu`, `self.conv2`, and `self.bn2` layers defined in the `__init__` method.\n",
    "2. Refer to Figure 2 in the [original ResNet paper](https://arxiv.org/abs/1512.03385) for the residual block structure.\n",
    "3. Apply batch normalization immediately after each convolutional layer.\n",
    "4. Remember to add the identity (input) to the output before the final activation.\n",
    "\n",
    "**TODO 2**: Complete the `self.downsample` part in the `__init__` method of the `ResidualBlock` class. <br>\n",
    "This block is used when the input dimensions need to be adjusted to match the output dimensions (when `use_downsample` is True).\n",
    "\n",
    "1. Create a `nn.Sequential` object for `self.downsample`\n",
    "2. The downsample block should contain:\n",
    "   - A convolutional layer with kernel size 1, stride equal to `in_stride`, and output channels equal to `out_channels`\n",
    "   - A batch normalization layer\n",
    "\n",
    "**Hint**: Remember to set `bias=False` for the convolutional layer as per the ResNet architecture.\n",
    "\n",
    "After implementing these components, run the provided test cells to verify your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_downsample=False):\n",
    "        super().__init__()\n",
    "\n",
    "        in_stride = 1\n",
    "        self.downsample = None\n",
    "        if use_downsample:\n",
    "            in_stride = 2\n",
    "            # TODO 2: Implement the downsample block\n",
    "            # (modify self.downsample to be a nn.Sequential object)\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=in_stride,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - x: (N, in_channels, H, W)\n",
    "\n",
    "        Returns:\n",
    "        - out:  (N, out_channels, H, W) if no downsample\n",
    "                (N, out_channels, H//2, W//2) if downsample\n",
    "        \"\"\"\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)  # (N, out_channels, H//2, W//2)\n",
    "        else:\n",
    "            identity = x  # (N, in_channels, H, W)\n",
    "\n",
    "        out = None\n",
    "        # TODO 1: Implement the forward pass of the residual block\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.TestResidualBlock.test_no_downsampling(ResidualBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.TestResidualBlock.test_downsampling(ResidualBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.TestResidualBlock.test_structure(ResidualBlock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"chp1-2-resnet34\"></a>\n",
    "## 1.2 ResNet34 Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've implemented the ResidualBlock, you'll use it to build a ResNet34. ResNet34 is a 34-layer deep convolutional neural network that utilizes residual connections to facilitate the training of very deep networks.\n",
    "\n",
    "Your task is to implement the ResNet34 class in the cell below. You can refer to Table 1 in the [ResNet paper](https://arxiv.org/abs/1512.03385) for the architecture details.\n",
    "\n",
    "**TODO**: Complete the `ResNet34` class by implementing the following:\n",
    "\n",
    "1. In the `__init__` method, use the `_make_layer` method to create `self.layer2`, `self.layer3`, and `self.layer4`.\n",
    "2. Implement the `forward` method.\n",
    "\n",
    "**Hints**:\n",
    "- Pay attention to the number of blocks and the input/output channels for each layer.\n",
    "- `self.layer2`, `self.layer3`, and `self.layer4` should use downsampling.\n",
    "\n",
    "After implementing the ResNet34 class, run the provided test cells to verify your implementation. These tests will check the forward pass, layer shapes, and overall structure of your ResNet34 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet34(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(ResidualBlock, 64, 64, 3, use_downsample=False)\n",
    "\n",
    "        # TODO: Implement self.layer2, self.layer3, self.layer4\n",
    "        # ResNet34 has 3, 4, 6, 3 residual blocks of 64, 128, 256, 512 channels\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, in_channels, out_channels, num_blocks, use_downsample):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            nn.Sequential(\n",
    "                block(in_channels, out_channels, use_downsample),\n",
    "                block(out_channels, out_channels),\n",
    "                block(out_channels, out_channels),\n",
    "                ...\n",
    "            )\n",
    "        \"\"\"\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(in_channels, out_channels, use_downsample)\n",
    "        )  # first block with possible downsample\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - x: (N, 3, 224, 224)\n",
    "\n",
    "        Returns:\n",
    "        - x: (N, num_classes)\n",
    "        \"\"\"\n",
    "        out = None\n",
    "\n",
    "        # TODO: Implement the forward pass of ResNet34\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.TestResNet34.test_forward(ResNet34)\n",
    "tests.TestResNet34.test_layer_shapes(ResNet34)\n",
    "tests.TestResNet34.test_structure(ResNet34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pre-trained Weights\n",
    "\n",
    "After passing the sanity checks, we can try loading the pre-trained weights to see if our implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_resent34(pretrained=False, progress=True, **kwargs):\n",
    "    model = ResNet34(**kwargs)\n",
    "    if pretrained:\n",
    "        weights = torchvision.models.ResNet34_Weights.DEFAULT\n",
    "        model.load_state_dict(weights.get_state_dict(progress=progress))\n",
    "        print(\"Successfully loaded pretrained weights.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "my_resnet34 = make_resent34(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"chp2-segmentation-fundamentals\"></a>\n",
    "# 2. Semantic Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of semantic segmentation is to assign a class label to each pixel in an image, for a predefined set of semantic classes. This section will introduce you to the key concepts of semantic segmentation without requiring any coding. Semantic segmentation is used in various applications, including autonomous driving, medical image analysis, and satellite imagery interpretation. Unlike image classification, which assigns a single label to an entire image, semantic segmentation provides a dense pixel-wise classification, allowing for a more detailed understanding of the image content.\n",
    "\n",
    "Key points to understand:\n",
    "- Each pixel in the output should be assigned a class label\n",
    "- The number of classes depends on the specific problem and dataset\n",
    "- The output is typically a mask with the same spatial dimensions as the input image \n",
    "\n",
    "Common metrics for evaluating semantic segmentation models include:\n",
    "1. **Mean Intersection over Union (mIoU)**: The average IoU across all classes, where IoU is the overlap between the predicted segmentation and the ground truth divided by their union.\n",
    "2. **Pixel Accuracy**: The percentage of correctly classified pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"chp2-1-dataset\"></a>\n",
    "## 2.1 Dataset Preparation and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll explore the dataset used for our semantic segmentation task and how it's prepared and visualized.\n",
    "\n",
    "### Dataset Overview\n",
    "\n",
    "We're using the PASCAL VOC 2012 dataset, a classic benchmark for semantic segmentation. It contains 1464 training and 1449 validation samples; each image is annotated with a pixel-precise map distinguishing between 20 foreground object classes and one background class. Additionally, some pixels may be labelled as 'void', meaning they do not have a clear label and should therefore be excluded from metric and loss computations.\n",
    "\n",
    "### Dataset Preparation\n",
    "\n",
    "We've implemented a custom `VOCDataset` class in `voc_dataset.py` to handle the loading and preprocessing of the dataset. \n",
    "\n",
    "### Visualization\n",
    "\n",
    "To aid in understanding our data and model predictions, we've implemented visualization functions in `visualization.py`. These functions will be used when logging images to TensorBoard during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample configuration\n",
    "config = ExperimentConfig(\"\", \"\", \"\", \"\", img_size=255, batch_size=6)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_loader, _ = create_dataloaders(config)\n",
    "\n",
    "# Get a batch of images and masks\n",
    "images, masks = next(iter(train_loader))\n",
    "\n",
    "# Create dummy predictions (just for visualization purposes)\n",
    "predictions = torch.randint(0, config.num_classes, masks.shape)\n",
    "\n",
    "# Visualize\n",
    "fig = visualize_segmentation(images, masks, predictions, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"chp2-2-metrics-loss\"></a>\n",
    "## 2.2 Metrics and Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will implement functions for training and evaluating semantic segmentation models.\n",
    "\n",
    "**TODO**: Implement the `calculate_pixel_accuracy` function below.\n",
    "This function should:\n",
    "1. Calculate the pixel-wise accuracy of the segmentation predictions.\n",
    "2. Ignore the void label in the accuracy calculation.\n",
    "3. Return the accuracy as a scalar tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pixel_accuracy(\n",
    "    pred: torch.Tensor, target: torch.Tensor, void_label: int\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculate the pixel accuracy for semantic segmentation.\n",
    "\n",
    "    Args:\n",
    "        pred: int tensor (of arbitrary shape) containing the predicted labels (from 0 to num_classes - 1)\n",
    "        target: int tensor containing the ground truth labels (from 0 to num_classes - 1)\n",
    "        void_label: label to ignore in accuracy calculation\n",
    "\n",
    "    Returns:\n",
    "        accuracy: scalar tensor of the pixel accuracy\n",
    "    \"\"\"\n",
    "    assert pred.size() == target.size(), \"pred and target must have the same shape\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Complete the `step_fn` method below. After implementing this function, run the cell below to run the sanity checks.\n",
    "\n",
    "The `step_fn` function should:\n",
    "1. Calculate the cross-entropy loss between the model outputs and the target labels.\n",
    "2. Compute the Mean Intersection over Union (mIoU) using the `calculate_miou` function.\n",
    "3. Calculate the pixel accuracy using the `calculate_pixel_accuracy` function you just implemented.\n",
    "4. Return a dictionary containing the loss, mIoU, and pixel accuracy.\n",
    "\n",
    "**Hints**:\n",
    "- Use `F.cross_entropy` to calculate the loss; it supports a `ignore_index` parameter to handle the void label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import calculate_miou\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class CNNTrainer(Trainer):\n",
    "    @staticmethod\n",
    "    def step_fn(\n",
    "        outputs: torch.Tensor, targets: torch.Tensor, void_label: int\n",
    "    ) -> dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Calculate the loss and mIoU given the model outputs and labels for a batch of images.\n",
    "\n",
    "        Args:\n",
    "            outputs: Predicted unnormalized logits. Shape (N, K, H, W), N is batch size, K is number of classes\n",
    "            targets: (N, H, W) where each value is between 0 and K-1\n",
    "            void_label: Label to ignore in loss calculation and mIoU\n",
    "\n",
    "        Returns: dict[str, torch.Tensor]\n",
    "            loss: scalar tensor\n",
    "            mIoU: scalar tensor\n",
    "            accuracy: scalar tensor\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"mIoU\": miou,\n",
    "            \"accuracy\": accuracy,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.TestStepFn.test_step(CNNTrainer.step_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"chp3-segmentation-models\"></a>\n",
    "# 3. Semantic Segmentation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic segmentation models typically utilize and encoder-decoder architecture. The encoder, also called _backbone_, is responsible for extracting features from the input image, while the decoder uses these features to produce the final segmentation map. Since semantic segmentation can be seen as pixel-wise classification, the decoder is sometimes also referred to as a classifier. The backbone is usually initialized with parameters of a pretrained image classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"chp3-1-segmentation-model\"></a>\n",
    "## 3.1 Segmentation Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will implement the `SemanticSegmentationModel` class, which combines a backbone and a decoder to create a complete semantic segmentation model.\n",
    "\n",
    "**TODO**: Complete the `forward` method in the `SemanticSegmentationModel` class below. After implementing this method, run the provided test cells for a sanity check.\n",
    "\n",
    "The `forward` method should:\n",
    "1. Pass the input through the backbone to extract features, which will be returned as a `FeatureDict`.\n",
    "2. Feed these features into the classifier to produce the segmentation output.\n",
    "\n",
    "**Hints**:\n",
    "- The `backbone` attribute is an instance of `BaseBackbone`, which returns a `FeatureDict` containing feature maps at different scales.\n",
    "- The `FeatureDict` includes keys like 'out2', 'out4', 'out8', and 'out16', representing feature maps at 1/2, 1/4, 1/8, and 1/16 of the input resolution respectively.\n",
    "- The `classifier` attribute is an instance of `BaseClassifier`, which takes the `FeatureDict` and produces the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_segmentation.backbones import BaseBackbone\n",
    "from semantic_segmentation.classifiers import BaseClassifier\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class SemanticSegmentationModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A model for semantic segmentation tasks.\n",
    "\n",
    "    Attributes:\n",
    "        backbone (BaseBackbone): The backbone network for feature extraction.\n",
    "        classifier (BaseClassifier): The classifier for segmentation.\n",
    "        config (ExperimentConfig): Configuration parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: BaseBackbone,\n",
    "        classifier: BaseClassifier,\n",
    "        config,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.classifier = classifier\n",
    "        self.config = config\n",
    "\n",
    "        if not self.config.train_backbone:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x: input tensor of shape (N, C, H, W)\n",
    "\n",
    "        Returns:\n",
    "            output segmentation map of shape (N, num_classes, H, W)\n",
    "        \"\"\"\n",
    "        # Passing the input through the backbone and classifier\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def configure_optimizers(self) -> optim.Optimizer:\n",
    "        if self.config.train_backbone:\n",
    "            return optim.Adam(\n",
    "                params=[\n",
    "                    {\"params\": self.backbone.parameters(), \"lr\": self.config.lr * 0.1},\n",
    "                    {\"params\": self.classifier.parameters(), \"lr\": self.config.lr},\n",
    "                ],\n",
    "                lr=self.config.lr,\n",
    "                weight_decay=self.config.weight_decay,\n",
    "            )\n",
    "        else:\n",
    "            return optim.Adam(\n",
    "                params=self.classifier.parameters(),\n",
    "                lr=self.config.lr,\n",
    "                weight_decay=self.config.weight_decay,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.TestSemanticSegmentationModel.test_forward(SemanticSegmentationModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"chp3-2-lite-raspp\"></a>\n",
    "## 3.2 Lite R-ASPP Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will implement the Lite R-ASPP (Lite Reduced Atrous Spatial Pyramid Pooling) Classifier, which is a lightweight version of the ASPP module (will be covered in later sections).\n",
    "\n",
    "The Lite R-ASPP module is designed to capture multi-scale context information while keeping the computational cost low, making it suitable for mobile and edge devices.\n",
    "\n",
    "**TODO**: Complete the `forward` method in the `LiteRASPPClassifier` class below. After implementing this method, run the provided test cells for a sanity check.\n",
    "\n",
    "Please refer the the figure 10 in the [MobileNetV3 paper](https://arxiv.org/abs/1905.02244) for the detailed architecture of the Lite R-ASPP module.\n",
    "\n",
    "**Hints**:\n",
    "- Refer to the paper for the detailed architecture of the Lite R-ASPP module.\n",
    "- We have already defined the necessary components for you. Your task is to use these components to implement the forward pass.\n",
    "- Use `x = F.interpolate(x, size=self.img_shape, mode=\"bilinear\", align_corners=False)` for any required upsampling operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from semantic_segmentation.backbones import ChannelDict, FeatureDict\n",
    "\n",
    "\n",
    "class LiteRASPPClassifier(BaseClassifier):\n",
    "    \"\"\"\n",
    "    Classifier using Lite R-ASPP module from MobileNetV3 paper\n",
    "    The paper uses hidden_size=128 and output_stride=16\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, backbone_channels: ChannelDict, config):\n",
    "        super().__init__()\n",
    "        self.img_shape = (config.img_size, config.img_size)\n",
    "        self.output_stride = config.output_stride\n",
    "\n",
    "        num_classes = config.num_classes\n",
    "        hidden_size = (\n",
    "            128\n",
    "            if config.classifier_hidden_size is None\n",
    "            else config.classifier_hidden_size\n",
    "        )\n",
    "\n",
    "        if self.output_stride == 16:\n",
    "            self.high_feature = \"out16\"\n",
    "            self.low_feature = \"out8\"\n",
    "        elif self.output_stride == 8:\n",
    "            self.high_feature = \"out8\"\n",
    "            self.low_feature = \"out4\"\n",
    "\n",
    "        high_channels = backbone_channels[self.high_feature]\n",
    "        low_channels = backbone_channels[self.low_feature]\n",
    "\n",
    "        # route 1 (high level feature map)\n",
    "        self.conv_module = nn.Sequential(\n",
    "            nn.Conv2d(high_channels, hidden_size, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.scale_module = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(high_channels, hidden_size, kernel_size=1, bias=False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.high_classifier = nn.Conv2d(hidden_size, num_classes, kernel_size=1)\n",
    "\n",
    "        # route 2\n",
    "        self.low_classifier = nn.Conv2d(low_channels, num_classes, kernel_size=1)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, features: FeatureDict) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: a dictionary containing the feature maps at different scales\n",
    "\n",
    "        Returns:\n",
    "            output segmentation map of shape (N, num_classes, self.image_shape[0], self.image_shape[1])\n",
    "        \"\"\"\n",
    "        high = features[self.high_feature]\n",
    "        low = features[self.low_feature]\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.TestClassifiers.test_lite_raspp_classifier(LiteRASPPClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Segmentation with MobileNetV3 and Lite R-ASPP\n",
    "\n",
    "Now that you've implemented the Lite R-ASPP Classifier, let's put it to the test by training a semantic segmentation model using MobileNetV3 as the backbone and Lite R-ASPP as the classifier.\n",
    "\n",
    "**Experiment Setup:**\n",
    "- Backbone: MobileNetV3-Large (pre-trained, frozen)\n",
    "- Classifier: Lite R-ASPP (your implementation)\n",
    "- Max Steps: 3000\n",
    "\n",
    "**TODO**: Run the following cell to run the experiment.\n",
    "\n",
    "**Note**: \n",
    "- If implemented correctly, the final **mIoU** for `img_size=225` should be around **0.4** and for `img_size=513` around **0.55**.\n",
    "- Running this experiment on CPU with `img_size`=225 should take around 30 minutes.\n",
    "- You can refer to Appendix A for performance benchmarks of off-the-shelf models to compare your results.\n",
    "\n",
    "\n",
    "To view the training progress, run the following command in your terminal:\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir=runs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_segmentation.backbones import MobileNetV3Backbone, ResNetBackbone\n",
    "# Set up the experiment configuration\n",
    "config = ExperimentConfig(\n",
    "    exp_name=\"VOC12\",\n",
    "    backbone_name=\"mobilenetv3_large\",\n",
    "    classifier_name=\"lite_raspp\",\n",
    "    img_size=513,\n",
    "    # img_size=225, # For CPU\n",
    "    # device=\"cpu\", # For CPU\n",
    ")\n",
    "\n",
    "# Create and run the experiment\n",
    "backbone = MobileNetV3Backbone(config)\n",
    "decoder = LiteRASPPClassifier(backbone.get_channels(), config)\n",
    "model = SemanticSegmentationModel(backbone, decoder, config)\n",
    "trainer = CNNTrainer(model, config)\n",
    "\n",
    "# Print model information\n",
    "print_model_info(model)\n",
    "\n",
    "# Run the experiment\n",
    "trainer.run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"chp3-4-aspp\"></a>\n",
    "## 3.4 Atrous Spatial Pyramid Pooling (ASPP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atrous convolution, also known as dilated convolution, is a method to enlarge the receptive field of convolutional layers without increasing the number of parameters or the amount of computation.\n",
    "\n",
    "In a standard convolution, the dilation rate $p$ is 1. Increasing the dilation rate is equivalent to inserting zeros between the values in the convolutional kernel, effectively increasing the filter size without adding parameters. For example:\n",
    "\n",
    "- A 3x3 convolution with p=1 (standard convolution) has a 3x3 receptive field\n",
    "- The same 3x3 convolution with p=2 yields an effective filter size of 5x5\n",
    "- With p=3, the effective filter size becomes 7x7\n",
    "\n",
    "This allows the network to capture wider context without losing resolution or increasing the number of parameters.\n",
    "\n",
    "For a more detailed explanation and visualizations of atrous convolution, refer to Chapter 5.1 of this [guide](https://arxiv.org/abs/1603.07285).\n",
    "\n",
    "In PyTorch, you can create dilated convolutions easily by specifying the `dilation` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section, you'll implement the Atrous Spatial Pyramid Pooling (ASPP) module, a module that captures multi-scale context information for improved segmentation performance.\n",
    "\n",
    "**TODO**: Implement the `forward` method in the `ASPP` class below. After implementing this method, run the provided test cells for a sanity check.\n",
    "\n",
    "The `ASPP` class structure is already provided for you. Your task is to complete the `forward` method. <br/>\n",
    "Please refer to Section 3.3 of the [DeepLabV3 paper](https://arxiv.org/abs/1706.05587) for the detailed architecture of the ASPP module.\n",
    "\n",
    "**Hints**:\n",
    "- The output feature maps produced by different atrous convolutions *don't* need to be resized. \n",
    "- However, the global pooling feature map should be upsampled before being concatenated with the atrous convolution outputs.\n",
    "- Use `F.interpolate` for upsampling operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_segmentation.classifiers import AtrousConv\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    \"\"\"\n",
    "    Atrous Spatial Pyramid Pooling module (and image pooling) from DeepLabV3\n",
    "\n",
    "    Args:\n",
    "        in_channels: number of input channels\n",
    "        out_channels: number of output channels\n",
    "        atrous_rates: a tuple of atrous rates for the atrous convolutions\n",
    "\n",
    "    As mentioned in the DeepLabV3 paper:\n",
    "        When output_stride = 16 -> atrous_rates = (6, 12, 18)\n",
    "        When output_stride = 8 -> atrous_rates = (12, 24, 36)\n",
    "        out_channels is 256 for both cases in the paper, though it's quite large for MobileNetV3.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, atrous_rates: Tuple[int], out_channels: int):\n",
    "        super(ASPP, self).__init__()\n",
    "\n",
    "        self.pyramid_convs = nn.ModuleList()\n",
    "\n",
    "        one_by_one_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.pyramid_convs.append(one_by_one_conv)\n",
    "\n",
    "        for rate in atrous_rates:\n",
    "            self.pyramid_convs.append(AtrousConv(in_channels, out_channels, rate))\n",
    "\n",
    "        self.image_pool = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # +2 for the image pool and the 1x1 conv\n",
    "        self.out_conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                out_channels * (len(atrous_rates) + 2),\n",
    "                out_channels,\n",
    "                kernel_size=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            (N, in_channels, H, W)\n",
    "\n",
    "        Returns:\n",
    "            (N, out_channels, H, W)\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.TestClassifiers.test_aspp(ASPP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"chp3-5-deeplabv3\"></a>\n",
    "## 3.5 DeepLabV3 Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you'll implement the DeepLabV3 classifier, which incorporates the ASPP module you created in the previous section.\n",
    "Please refer to the [DeepLabV3 paper](https://arxiv.org/abs/1706.05587) for the architecture of the DeepLabV3 model.\n",
    "\n",
    "**TODO**: Complete the `forward` method in the `DeepLabV3Classifier` class below. After implementing this method, run the provided test cells to run a sanity check.\n",
    "\n",
    "**Hints**:\n",
    "- The output needs to be bilinearly upsampled to match the input resolution using `F.interpolate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_segmentation.classifiers import determine_atrous_rates\n",
    "\n",
    "\n",
    "class DeepLabV3Classifier(BaseClassifier):\n",
    "    \"\"\"\n",
    "    Classifer with similar architecture to DeepLabV3\n",
    "    When using MobileNetV3 as the backbone, output_stride should be 16 for better performance.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, backbone_channels: ChannelDict, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.img_shape = (config.img_size, config.img_size)\n",
    "        self.output_stride = config.output_stride\n",
    "\n",
    "        num_classes = config.num_classes\n",
    "        hidden_size = (\n",
    "            64\n",
    "            if config.classifier_hidden_size is None\n",
    "            else config.classifier_hidden_size\n",
    "        )\n",
    "\n",
    "        atrous_rates = determine_atrous_rates(self.output_stride)\n",
    "\n",
    "        if self.output_stride == 16:\n",
    "            in_channels = backbone_channels[\"out16\"]\n",
    "        elif self.output_stride == 8:\n",
    "            in_channels = backbone_channels[\"out8\"]\n",
    "\n",
    "        self.aspp = ASPP(in_channels, atrous_rates, hidden_size)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(hidden_size, hidden_size, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden_size, num_classes, 1),\n",
    "        )\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, features: FeatureDict) -> torch.Tensor:\n",
    "        if self.output_stride == 16:\n",
    "            feature = features[\"out16\"]\n",
    "        elif self.output_stride == 8:\n",
    "            feature = features[\"out8\"]\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                f\"Output stride of {self.output_stride} is not supported\"\n",
    "            )\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.TestClassifiers.test_deeplabv3_classifier(DeepLabV3Classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Segmentation with DeepLabV3\n",
    "\n",
    "Now that you've implemented the DeepLabV3 classifier, let's train it for semantic segmentation. This experiment will use a MobilenetV3 backbone with the DeepLabV3 classifier you just implemented.\n",
    "\n",
    "**Experiment Setup:**\n",
    "- Backbone: MobileNetV3-Large (pre-trained, frozen)\n",
    "- Classifier: DeepLabV3 (your implementation)\n",
    "- Max Steps: 3000\n",
    "\n",
    "**TODO**: Run the following cell to run the experiment.\n",
    "\n",
    "**Note**: \n",
    "- If implemented correctly, the final **mIoU** for `img_size=225` should be around **0.44** and `img_size=513` should be around **0.6**.\n",
    "- You can compare these results with the benchmarks provided in Appendix A to see how your implementation performs relative to off-the-shelf models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the experiment configuration\n",
    "config = ExperimentConfig(\n",
    "    exp_name=\"VOC12\",\n",
    "    backbone_name=\"mobilenetv3_large\",\n",
    "    classifier_name=\"deeplabv3\",\n",
    "    img_size=513,\n",
    "    output_stride=16,\n",
    "    classifier_hidden_size=64,\n",
    ")\n",
    "\n",
    "# Create and run the experiment\n",
    "backbone = MobileNetV3Backbone(config)\n",
    "decoder = DeepLabV3Classifier(backbone.get_channels(), config)\n",
    "model = SemanticSegmentationModel(backbone, decoder, config)\n",
    "trainer = CNNTrainer(model, config)\n",
    "\n",
    "# Print model information\n",
    "print_model_info(model)\n",
    "\n",
    "# Run the experiment\n",
    "trainer.run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"chp3-6-deeplabv3-resnet50\"></a>\n",
    "## 3.6 DeepLabV3 with ResNet50 Backbone (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we'll combine a powerful ResNet50 backbone with the DeepLabV3 architecture. This setup closely mirrors the original DeepLabV3 model (ResNet50 version), with the only difference being the exclusion of an auxiliary classifier.\n",
    "\n",
    "**Experiment Setup**:\n",
    "- Backbone: ResNet50 (pre-trained, frozen)\n",
    "- Classifier: DeepLabV3 (your implementation)\n",
    "- Max Steps: 3000\n",
    "- Image size = 513\n",
    "\n",
    "**TODO**: Run the following cell to run the experiment.\n",
    "\n",
    "**Note**: \n",
    "- The final **mIoU** should be around **0.67**.\n",
    "- Running this experiment takes roughly 2 hours on a GPU.\n",
    "- You can refer to Appendix A to compare this result with the performance of similar off-the-shelf models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the experiment configuration\n",
    "config = ExperimentConfig(\n",
    "    exp_name=\"VOC12\",\n",
    "    backbone_name=\"resnet50\",\n",
    "    classifier_name=\"deeplabv3\",\n",
    "    img_size=513,\n",
    "    output_stride=8,\n",
    "    classifier_hidden_size=256,\n",
    ")\n",
    "\n",
    "# Create and run the experiment\n",
    "backbone = ResNetBackbone(config)\n",
    "decoder = DeepLabV3Classifier(backbone.get_channels(), config)\n",
    "model = SemanticSegmentationModel(backbone, decoder, config)\n",
    "trainer = CNNTrainer(model, config)\n",
    "\n",
    "# Print model information\n",
    "model = trainer.model\n",
    "print_model_info(model)\n",
    "\n",
    "# Run the experiment\n",
    "trainer.run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"appendix\"></a>\n",
    "# A. Appendix: Performance Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give you an idea of what to expect when training semantic segmentation models, here are benchmarks for some off-the-shelf models on the PASCAL VOC 2012 validation set. These results are from pre-trained models available in torchvision, tested at various input resolutions using our own evaluation pipeline. It's important to note that these numbers may differ from those reported in papers due to differences in evaluation methodologies - in particular, we employ much shorter training schedules, less data, and no augmentations.\n",
    "\n",
    "You can reproduce these benchmarks using the `utils.eval_official()`.\n",
    "\n",
    "\n",
    "### Mean Intersection over Union (mIoU)\n",
    "\n",
    "| Backbone          | Classifier | Model Size | 65x65  | 129x129 | 225x225 | 513x513 |\n",
    "|-------------------|------------|------------|--------|---------|---------|---------|\n",
    "| MobileNetV3_Large | LRASPP     | 3.22M      | 0.0507 | 0.1440  | 0.4077  | 0.5735  |\n",
    "| ResNet50          | DeepLabV3  | 42.00M     | 0.1753 | 0.4360  | 0.5966  | 0.6929  |\n",
    "| ResNet101         | DeepLabV3  | 61.00M     | 0.2052 | 0.4514  | 0.6195  | 0.7064  |\n",
    "\n",
    "### Validation Loss\n",
    "\n",
    "| Backbone          | Classifier | Model Size | 65x65  | 129x129 | 225x225 | 513x513 |\n",
    "|-------------------|------------|------------|--------|---------|---------|---------|\n",
    "| MobileNetV3_Large | LRASPP     | 3.22M      | 2.2028 | 1.1351  | 0.4390  | 0.2519  |\n",
    "| ResNet50          | DeepLabV3  | 42.00M     | 1.2792 | 0.4682  | 0.2564  | 0.1815  |\n",
    "| ResNet101         | DeepLabV3  | 61.00M     | 1.1571 | 0.4336  | 0.2342  | 0.1689  |\n",
    "\n",
    "### Pixel Accuracy\n",
    "\n",
    "| Backbone          | Classifier | Model Size | 65x65  | 129x129 | 225x225 | 513x513 |\n",
    "|-------------------|------------|------------|--------|---------|---------|---------|\n",
    "| MobileNetV3_Large | LRASPP     | 3.22M      | 0.6553 | 0.7263  | 0.8565  | 0.9128  |\n",
    "| ResNet50          | DeepLabV3  | 42.00M     | 0.7249 | 0.8557  | 0.9125  | 0.9378  |\n",
    "| ResNet101         | DeepLabV3  | 61.00M     | 0.7392 | 0.8635  | 0.9195  | 0.9435  |\n",
    "\n",
    "\n",
    "You can refer to this table throughout the assignment to compare your implementation's performance against these benchmarks. Keep in mind that your results may differ due to our simplified training protocol (frozen backbone, no auxiliary classifier, limited hyperparameter tuning, etc.). However, this table provides a good reference point for what performance you can expect from models with similar architectures and sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "utils.eval_official([513])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"references\"></a>\n",
    "# B. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following papers are either directly used in this assignment or highly correlated with the concepts we've covered:\n",
    "\n",
    "1. [ResNet](https://arxiv.org/abs/1512.03385)\n",
    "2. [DeepLabV1](https://arxiv.org/abs/1412.7062)\n",
    "3. [DeepLabV2](https://arxiv.org/abs/1606.00915)\n",
    "4. [DeepLabV3](https://arxiv.org/abs/1706.05587)\n",
    "5. [MobileNet](https://arxiv.org/abs/1704.04861)\n",
    "6. [MobileNetV2](https://arxiv.org/abs/1801.04381)\n",
    "7. [MobileNetV3](https://arxiv.org/abs/1905.02244)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baidl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
