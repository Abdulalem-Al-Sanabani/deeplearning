{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BridgingAI Logo](../bridgingai_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Exercise 6.2: Efficient Fine-Tuning \n",
    "\n",
    "---\n",
    "1. [Finetuning pretrained models for Image Captioning](#implementation)\n",
    "\n",
    "2. [Experiments](#experiments)\n",
    "   \n",
    "3. [References](#references)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoModel\n",
    "\n",
    "from config import ExperimentConfig\n",
    "from trainer import Trainer\n",
    "from utils import compute_bleu\n",
    "\n",
    "# silence warnings and avoid deadlocks due to HF tokenizer\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, your will train an image captioning model by integrating a vision backbone with a language model and fine-tuning it using LoRA. You will use the [Flickr30k](https://huggingface.co/datasets/nlphuji/flickr30k) dataset for this task. Running the experiments in this notebook requires around 6GB of disk space for the dataset and the model checkpoints, and a GPU to achieve reasonable training times.\n",
    "\n",
    "The vision backbone is a ViT that transforms images into a sequence of embeddings, then the language model will process both the image embeddings and the text embeddings to generate captions. This is similar to translation tasks, where the source text is replaced by the image embeddings.\n",
    "\n",
    "We choose [AIMv2](https://huggingface.co/apple/aimv2-large-patch14-224) as the vision backbone, and [SmolLM2](https://huggingface.co/HuggingFaceTB/SmolLM2-135M) as the language model. AIMv2 has 300M parameters and SmolLM2 has 135M parameters (same as the smallest GPT-2 model). The overall architecture closely resembles Figure 1 in the [PaliGemma paper](https://arxiv.org/abs/2407.07726), where the output of the vision backbone is projected to the same dimension as the language model via a linear layer. The combined embeddings are then processed by the language model to produce captions.\n",
    "\n",
    "This assignment demonstrates that:\n",
    "1. With transformers, it is trivial to combine different modalities (images and text).\n",
    "2. LoRA enables us to fine-tune large models even with limited resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Implementation Tasks <a id=\"implementation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A critical aspect of combining the vision backbone with the language model is defining an appropriate attention mask between the image embeddings and the text embeddings. This attention mask is illustrated in Figure 3 of [GIT: A Generative Image-to-text Transformer for Vision and Language](https://arxiv.org/abs/2205.14100). In this setup, image embeddings attend only to other image embeddings and not to the text, while text embeddings attend to all image embeddings as well as the preceding text embeddings.\n",
    "\n",
    "This masking strategy is widely used in tasks such as image captioning and visual question answering. Many well-known models, including PaliGemma, adopt this approach to effectively handle the interplay between vision and language representations.\n",
    "\n",
    "---\n",
    "\n",
    "**TODO**: \n",
    "- Complete the `image_text_attention_mask` method in `MaskMixin` class to implement the attention mask between the image embeddings and the text embeddings.\n",
    "- Note that you should use the constants `ATTEND` and `IGNORE` to fill the attention mask.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskMixin:\n",
    "    \"\"\"Handles generation of attention masks for huggingface (especially LLaMA) models that accept image and text embeddings.\n",
    "\n",
    "    Some notes:\n",
    "    - Attention mask produced by HF tokenizer: torch.int64, {0, 1}\n",
    "    - Attention mask accepted by PyTorch sdpa attention: float or bool\n",
    "    - But attention mask used by HF LLaMA models: float, {-inf, 0} since it's additive\n",
    "    \"\"\"\n",
    "\n",
    "    ATTEND = 0\n",
    "    IGNORE = float(\"-inf\")\n",
    "\n",
    "    @staticmethod\n",
    "    def check_mask(mask):\n",
    "        assert (\n",
    "            mask.dtype == torch.float32\n",
    "        ), f\"Attention mask must be float32, got {mask.dtype}\"\n",
    "        assert torch.all(\n",
    "            (mask == MaskMixin.ATTEND) | (mask == MaskMixin.IGNORE)\n",
    "        ), f\"Attention mask must be {MaskMixin.ATTEND} or {MaskMixin.IGNORE}, got {mask.unique()}\"\n",
    "\n",
    "    @staticmethod\n",
    "    def image_text_attention_mask(batch_size, img_len, text_len, device):\n",
    "        \"\"\"Create an attention mask for [image, text] -> [image, text] attention.\n",
    "\n",
    "        Args:\n",
    "            batch_size: int, the number of samples in the batch\n",
    "            img_len: int, the length of the image token sequence\n",
    "            text_len: int, the maximum length of the text token sequence\n",
    "\n",
    "        Returns:\n",
    "            attn_mask: float tensor of shape (batch_size, 1, img_len+text_len, img_len+text_len)\n",
    "                It looks like this (for img_len=2 and text_len=3):\n",
    "                    [ 0,  0, -inf, -inf, -inf]\n",
    "                    [ 0,  0, -inf, -inf, -inf]\n",
    "                    [ 0,  0,    0, -inf, -inf]\n",
    "                    [ 0,  0,    0,    0, -inf]\n",
    "                    [ 0,  0,    0,    0,    0]\n",
    "                where image can see all image but cannot see text, and text can see image,\n",
    "                and current text can see all previous text.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        attn_mask = attn_mask.to(device)\n",
    "        return attn_mask\n",
    "\n",
    "    @staticmethod\n",
    "    def padding_mask(batch_size, img_len, text_padding_mask, device):\n",
    "        \"\"\"Extend the text padding mask to include the image tokens (which are always attended to).\n",
    "\n",
    "        Args:\n",
    "            batch_size: int, the number of samples in the batch\n",
    "            img_len: int, the length of the image token sequence\n",
    "            text_padding_mask: float tensor of shape (batch_size, text_seq_len)\n",
    "\n",
    "        Returns:\n",
    "            padding_mask: tensor of shape (batch_size, 1, 1, img_len+text_len)\n",
    "        \"\"\"\n",
    "        assert text_padding_mask.device == device, \"Device mismatch\"\n",
    "        MaskMixin.check_mask(text_padding_mask)\n",
    "\n",
    "        image_padding_mask = torch.full((batch_size, 1, 1, img_len), MaskMixin.ATTEND)\n",
    "        image_padding_mask = image_padding_mask.float().to(device)\n",
    "\n",
    "        text_padding_mask = text_padding_mask[:, None, None, :]\n",
    "        padding_mask = torch.cat([image_padding_mask, text_padding_mask], dim=-1)\n",
    "        padding_mask = padding_mask\n",
    "        return padding_mask\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_float(mask):\n",
    "        \"\"\"Converts the attention mask to float and expected values for LLaMA models.\"\"\"\n",
    "        if mask.dtype == torch.int64:\n",
    "            return_mask = mask.clone().float()\n",
    "            return_mask[mask == 1] = MaskMixin.ATTEND\n",
    "            return_mask[mask == 0] = MaskMixin.IGNORE\n",
    "            MaskMixin.check_mask(return_mask)\n",
    "            return return_mask\n",
    "        elif mask.dtype == torch.bool:\n",
    "            return_mask = mask.clone().float()\n",
    "            return_mask[mask == 1] = MaskMixin.ATTEND\n",
    "            return_mask[mask == 0] = MaskMixin.IGNORE\n",
    "            MaskMixin.check_mask(return_mask)\n",
    "            return return_mask\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown mask dtype: {mask.dtype}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def combine_masks(mask1, mask2):\n",
    "        \"\"\"Encode the logical AND operation between two masks.\"\"\"\n",
    "        MaskMixin.check_mask(mask1)\n",
    "        MaskMixin.check_mask(mask2)\n",
    "        return mask1 + mask2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(nn.Module, MaskMixin):\n",
    "    \"\"\"Image captioning model that use a language model to process both image embeddings and text tokens.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.train_encoder = config.train_encoder\n",
    "        self.processor = config.processor\n",
    "\n",
    "        self.image_encoder = AutoModel.from_pretrained(\n",
    "            config.image_encoder_checkpoint,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "        self.decoder = AutoModelForCausalLM.from_pretrained(\n",
    "            config.decoder_checkpoint,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "        # Extract the text embedding layer from the language model\n",
    "        self.text_embedding = self.decoder.get_input_embeddings()\n",
    "\n",
    "        # Project the image embeddings to the same size as the text embeddings\n",
    "        img_hidden_size = config.encoder_hidden_size\n",
    "        text_hidden_size = config.decoder_hidden_size\n",
    "        self.image_out_proj = nn.Linear(img_hidden_size, text_hidden_size)\n",
    "        print(\"Image encoder hidden size:\", img_hidden_size)\n",
    "        print(\"Decoder hidden size:\", text_hidden_size)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def encode_images(self, images=None, image_features=None):\n",
    "        \"\"\"Encode images or process pre-computed features.\n",
    "\n",
    "        Args:\n",
    "            images: Optional float tensor of shape (batch_size, 3, height, width)\n",
    "            image_features: Optional float tensor of shape (batch_size, seq_len, hidden_size). Must be the output from the same image encoder architecture\n",
    "\n",
    "        Returns:\n",
    "            image_embeds: float tensor of shape (batch_size, seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "        if images is not None and image_features is not None:\n",
    "            raise ValueError(\"Only one of images or image_features should be provided\")\n",
    "\n",
    "        if images is None and image_features is None:\n",
    "            raise ValueError(\"Either images or image_features must be provided\")\n",
    "\n",
    "        if image_features is not None:\n",
    "            return image_features\n",
    "\n",
    "        if images is not None:\n",
    "            with torch.set_grad_enabled(self.train_encoder):\n",
    "                image_features = self.image_encoder(\n",
    "                    pixel_values=images\n",
    "                ).last_hidden_state\n",
    "\n",
    "            if image_features.dim() == 4:\n",
    "                # this means (B, C, H, W). We need to flatten it to (B, H*W, C)\n",
    "                B, C, H, W = image_features.shape\n",
    "                image_features = image_features.permute(0, 2, 3, 1)\n",
    "                image_features = image_features.reshape(B, H * W, C)\n",
    "                image_features = image_features.contiguous()\n",
    "\n",
    "            img_embeds = self.image_out_proj(image_features)\n",
    "\n",
    "            return img_embeds\n",
    "\n",
    "    def forward(self, texts, text_padding_mask, images=None, image_features=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts: int tensor of shape (batch_size, seq_len)\n",
    "            text_padding_mask: tensor of shape (batch_size, seq_len) generted by the tokenizer\n",
    "            images: flaot tensor of shape (batch_size, 3, 224, 224)\n",
    "            image_features: float tensor of shape (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            logits: float tensor of shape (batch_size, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        # encode images\n",
    "        image_embeds = self.encode_images(images, image_features)\n",
    "\n",
    "        # encode texts\n",
    "        text_embeds = self.text_embedding(texts)\n",
    "\n",
    "        # concatenate image and text embeddings\n",
    "        inputs_embeds = torch.cat([image_embeds, text_embeds], dim=1)\n",
    "\n",
    "        # create attention mask\n",
    "        B = image_embeds.shape[0]\n",
    "        img_len = image_embeds.shape[1]\n",
    "        text_len = text_embeds.shape[1]\n",
    "        attn_mask = self.image_text_attention_mask(B, img_len, text_len, self.device)\n",
    "        # logical and\n",
    "        text_padding_mask = self.convert_to_float(text_padding_mask)\n",
    "        padding_mask = self.padding_mask(B, img_len, text_padding_mask, self.device)\n",
    "        attn_mask = self.combine_masks(attn_mask, padding_mask)\n",
    "\n",
    "        # forward pass through the decoder\n",
    "        outputs = self.decoder(inputs_embeds=inputs_embeds, attention_mask=attn_mask)\n",
    "        return outputs.logits[:, img_len:, :].contiguous()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, images, max_length=50):\n",
    "        \"\"\"Given a batch of images, generate captions using greedy decoding.\n",
    "\n",
    "        Args:\n",
    "            images: list of PIL images\n",
    "            max_length: int, the maximum length of the generated sequence\n",
    "\n",
    "        Returns:\n",
    "            captions: list of predicted captions strings\n",
    "            full_text: list of predicted captions strings with special tokens\n",
    "        \"\"\"\n",
    "        B = len(images)\n",
    "        # Convert images and start tokens to tensors\n",
    "        start_token = [self.processor.tokenizer.bos_token for _ in range(B)]\n",
    "        inputs = self.processor(images=images, text=start_token, return_tensors=\"pt\")\n",
    "\n",
    "        pixel_values = inputs[\"pixel_values\"].to(self.device)\n",
    "        generated_ids = inputs[\"input_ids\"].to(self.device)\n",
    "        text_padding_mask = inputs[\"attention_mask\"].to(self.device)\n",
    "\n",
    "        # encode images\n",
    "        image_embeds = self.encode_images(images=pixel_values)\n",
    "\n",
    "        # maintain a list of finished sequences\n",
    "        is_finished = torch.zeros((B, 1), dtype=torch.bool, device=self.device)\n",
    "\n",
    "        # generate captions\n",
    "        for i in range(max_length):\n",
    "            logits = self.forward(\n",
    "                texts=generated_ids,\n",
    "                text_padding_mask=text_padding_mask,\n",
    "                image_features=image_embeds,\n",
    "            )\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            next_token_id = next_token_logits.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "            # update finished sequences\n",
    "            is_finished |= next_token_id == self.processor.tokenizer.eos_token_id\n",
    "            next_token_id[is_finished] = self.processor.tokenizer.pad_token_id\n",
    "\n",
    "            generated_ids = torch.cat([generated_ids, next_token_id], dim=1)\n",
    "            _next_mask = (~is_finished).to(torch.int64)\n",
    "            text_padding_mask = torch.cat([text_padding_mask, _next_mask], dim=1)\n",
    "\n",
    "            if is_finished.all():\n",
    "                break\n",
    "\n",
    "        # cut off the start token\n",
    "        generated_ids = generated_ids[:, 1:]\n",
    "\n",
    "        # decode the generated token ids\n",
    "        captions = self.processor.tokenizer.batch_decode(\n",
    "            generated_ids, skip_special_tokens=True\n",
    "        )\n",
    "        return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "\n",
    "def build_model(config):\n",
    "    # configures the model for finetuning with LoRA\n",
    "    model = ImageCaptioningModel(config)\n",
    "    if config.train_decoder is False:\n",
    "        # Freeze the decoder\n",
    "        model.decoder.requires_grad_(False)\n",
    "    elif config.decoder_lora_modules is not None:\n",
    "        lora_config = LoraConfig(\n",
    "            r=config.lora_r,\n",
    "            lora_alpha=config.lora_alpha,\n",
    "            target_modules=config.decoder_lora_modules,\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "        )\n",
    "        model.decoder = get_peft_model(model.decoder, lora_config)\n",
    "    else:\n",
    "        print(\"No LoRA modules for decoder provided\")\n",
    "\n",
    "    if config.train_encoder is False:\n",
    "        # Freeze the image encoder\n",
    "        model.image_encoder.requires_grad_(False)\n",
    "    elif config.image_encoder_lora_modules is not None:\n",
    "        # Train the image encoder modules with LoRA\n",
    "        lora_config = LoraConfig(\n",
    "            r=config.lora_r,\n",
    "            lora_alpha=config.lora_alpha,\n",
    "            target_modules=config.image_encoder_lora_modules,\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "        )\n",
    "        model.image_encoder = get_peft_model(model.image_encoder, lora_config)\n",
    "    else:\n",
    "        print(\"No LoRA modules for image encoder provided\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Experiments <a id=\"experiments\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will train the image captioning model using the Flickr30k dataset. Run the following cells to fine-tune the model using LoRA. If your implementation is correct, you should achieve a validation loss of around 2.5 after 10k steps, and 2.1 after 40k steps.\n",
    "\n",
    "Note that this experiement takes a long time to run. Training the model for 10k steps takes about 2 hours on a GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ExperimentConfig(\n",
    "    \"aimv2_smollm2_lora\",\n",
    "    encoder_hidden_size=1024,\n",
    "    decoder_hidden_size=576,\n",
    "    train_encoder=True,\n",
    "    image_encoder_lora_modules=[\n",
    "        \"qkv\",\n",
    "        \"proj\",\n",
    "    ],\n",
    "    train_decoder=True,\n",
    "    decoder_lora_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    compute_bleu=False,\n",
    "    max_steps=40000,\n",
    "    save_model=True,\n",
    ")\n",
    "# if you have a powerful GPU, you can also try to finetune the full encoder\n",
    "model = build_model(config)\n",
    "trainer = Trainer(model, config)\n",
    "trainer.run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the BLEU score of a trained model. If trained for 40k steps, the BLEU score should be above 30. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to load the saved model\n",
    "# trainer.model.load_state_dict(torch.load(f\"model_base.pth\", weights_only=True))\n",
    "trainer.model.eval()\n",
    "bleu_score = compute_bleu(trainer.model, trainer.val_loader)\n",
    "print(f\"BLEU score: {bleu_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. References <a id=\"references\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [PaliGemma: Vision-and-Language Pretraining for Image Captioning](https://arxiv.org/abs/2407.07726)\n",
    "- [GIT: A Generative Image-to-text Transformer for Vision and Language](https://arxiv.org/abs/2205.14100)\n",
    "- [AIMv2](https://huggingface.co/apple/aimv2-large-patch14-224)\n",
    "- [SmolLM2](https://huggingface.co/HuggingFaceTB/SmolLM2-135M)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baidl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
