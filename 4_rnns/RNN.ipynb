{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BridgingAI Logo](../bridgingai_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Exercise 4: Recurrent Neural Networks for Language Modeling and Neural Machine Translation\n",
    "---\n",
    "1. [Character-Level Language Modeling](#lm)\n",
    "<br/> &#9; 1.1. [Character-Level Tokenization](#char-tokenization)\n",
    "<br/> &#9; 1.2. [Training Input and Target](#input_target)\n",
    "\n",
    "2. [RNN Implementation](#rnn-implementation)\n",
    "<br/> &#9; 2.1. [RNN](#rnn)\n",
    "<br/> &#9; 2.2. [LSTM (Optional)](#lstm) \n",
    "\n",
    "3. [Experiment: Character-Level Language Modeling](#experiment-lm)\n",
    "\n",
    "4. [Neural Machine Translation](#neural-machine-translation)\n",
    "<br/> &#9; 4.1. [Dataset Preparation](#dataset-preparation)\n",
    "<br/> &#9; 4.2. [Tokenization](#tokenization)\n",
    "<br/> &#9; 4.3. [Batching and Padding](#batching-and-padding)\n",
    "<br/> &#9; 4.4. [BLEU](#bleu)\n",
    "\n",
    "5. [NMT Implementation: Building Seq2Seq](#implementation-nmt)\n",
    "<br/> &#9; 5.1. [Encoder](#encoder)\n",
    "<br/> &#9; 5.2. [Decoder](#decoder)\n",
    "\n",
    "6. [Experiment: Neural Machine Translation](#experiment-nmt)\n",
    "\n",
    "7. [Questions](#questions)\n",
    "\n",
    "8. [References](#references)\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "import math\n",
    "\n",
    "from tests.lm_sanity_checks import TestRNN, TestLSTM\n",
    "from tests.nmt_sanity_checks import TestNMTEncoder, TestNMTDecoder\n",
    "from configs.lm_config import LMExperimentConfig\n",
    "from configs.nmt_config import NMTExperimentConfig\n",
    "from trainers.lm_trainer import LMTrainer\n",
    "from trainers.nmt_trainer import NMTTrainer\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks (RNNs) represent the foundational architecture for processing sequential data through maintained hidden states. While vanilla RNNs and LSTMs have largely given way to attention-based architectures or state-space models, their core principle of maintaining and updating state information persists in many modern architectures. In particular, many modern architectures allow easier parallelization of the training process, which makes it easier to scale to large models and large datasets. Nevertheless, the general principle of recurrent neural networks - reuse of model weights across the sequence - persists.\n",
    "\n",
    "This assignment explores sequence modeling tasks with RNNs:\n",
    "- Implement a vanilla RNN in PyTorch and train it for character-level language modeling on Shakespeare's texts\n",
    "- Build an encoder-decoder (Seq2Seq) model for German-to-English translation using the Multi30k dataset\n",
    "- Work hands-on with core NLP concepts including tokenization strategies, padding mechanisms, and evaluation metrics for sequence tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lm\"></a>\n",
    "# 1. Character-Level Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language modeling is a fundamental task in NLP where we aim to predict the next token in a sequence given the previous tokens. A token might be a single character, a word, a word piece or even a piece of a whole sentence. In this assignment, we'll focus on character-level language modeling, where tokens are individual characters rather than words.\n",
    "\n",
    "Consider a text sequence \"Hello\". During training, a character-level language model would:\n",
    "0. Start with the special \"start-of-sentence token\" and predict \"H\".\n",
    "1. Take \"H\" and predict \"e\"\n",
    "2. Take \"He\" and predict \"l\" \n",
    "3. Take \"Hel\" and predict \"l\"\n",
    "4. Take \"Hell\" and predict \"o\"\n",
    "\n",
    "More formally, given a sequence of characters $x_1, x_2, ..., x_T$, the goal of a character-level language model is to predict the probability distribution of the next character $x_{T+1}$:\n",
    "\n",
    "$$P(x_{T+1} | x_1, x_2, ..., x_T)$$\n",
    "\n",
    "Now let's visualize the data we'll be working with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"char-tokenization\"></a>\n",
    "## 1.1. Character-Level Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization** refers to the process of breaking down a text into smaller units, such as words or subwords. \n",
    "\n",
    "Before training a language model, we need to convert text into numbers that our neural network can process. In character-level modeling, we assign a unique ID to each character in our vocabulary, including spaces and punctuation. Since we know which characters to expect (our \"vocabulary\"), it is fairly easy to create the tokenizer: just enumerate the closed set of allowed characters. More sophisticated tokenizers need to be trained to learn a suitable vocabulary from some dataset.\n",
    "\n",
    "The pipeline for processing text data consists of the following stages:\n",
    "$$\n",
    "\\text{Text} \\rightarrow \n",
    "\\text{Tokens} \\rightarrow \n",
    "\\text{Token IDs} \\rightarrow \n",
    "\\text{Vectors} \\rightarrow \n",
    "\\text{\\{feed to the model\\}}...\n",
    "$$\n",
    "\n",
    "Let's look at how the tokenization works with a simple example. Each character gets mapped to an integer ID, and special tokens like space (' ') are included in the vocabulary. The tokenizer can thus convert text to IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_tokenization(config, text_string):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text and displays character (token) to ID mappings.\n",
    "    \"\"\"\n",
    "    # Tokenize the input text\n",
    "    token_ids = config.tokenizer.encode(text_string)\n",
    "\n",
    "    # Display input text and token IDs\n",
    "    print(f\"Input Text: '{text_string}'\")\n",
    "    print(f\"Token IDs: {token_ids}\\n\")\n",
    "\n",
    "    # Print the token to ID mapping\n",
    "    print(\"Token → ID:\")\n",
    "    print(\"-\" * 15)\n",
    "    for token_id in sorted(set(token_ids)):\n",
    "        char = config.tokenizer.id2char[token_id]\n",
    "        print(f\"{repr(char):<5} → {token_id}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "text_string = \"Now is the winter\"\n",
    "config = LMExperimentConfig(\"pytorch_rnn\")\n",
    "display_tokenization(config, text_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"input_target\"></a>\n",
    "## 1.2. Training Input and Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For language modeling, each training example consists of an input sequence and a target sequence. The target sequence is simply the input sequence shifted by one position - we want the model to predict the next character at each position.\n",
    "\n",
    "As shown below, the dataset is organized into batches where:\n",
    "- Input shape is (batch_size, sequence_length) \n",
    "- Target shape is (batch_size, sequence_length)\n",
    "- Each target sequence is offset by one character from its input sequence\n",
    "\n",
    "During training, the model processes the input sequence character by character, trying to predict the next character at each step. The loss is calculated by comparing these predictions against the target sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_utils.nlp.lm.utils import create_lm_dataloaders\n",
    "\n",
    "\n",
    "def display_dataloader_sample_with_decoding(config):\n",
    "    \"\"\"\n",
    "    Displays a sample input-target pair from the dataset, batch shapes,\n",
    "    and includes the decoded text for both input and target sequences.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(0)\n",
    "    # Create data loaders\n",
    "    train_loader, _ = create_lm_dataloaders(config, config.seq_len)\n",
    "    train_dataset = train_loader.dataset\n",
    "\n",
    "    # Get a sample input-target pair\n",
    "    sample_input, sample_target = train_dataset[0]\n",
    "    sample_input = sample_input.tolist()\n",
    "    sample_target = sample_target.tolist()\n",
    "\n",
    "    # Decode the sequences back to text\n",
    "    decoded_input = config.tokenizer.decode(sample_input)\n",
    "    decoded_target = config.tokenizer.decode(sample_target)\n",
    "\n",
    "    # Print the sample input, target, and decoded text\n",
    "    print(\"Sample Input (IDs): \\t\", sample_input)\n",
    "    print(\"Sample Target (IDs): \\t\", \" \" * 3, sample_target)\n",
    "    print(\"\\nDecoded Input:   \", decoded_input)\n",
    "    print(\"Decoded Target:  \", decoded_target)\n",
    "\n",
    "    # Fetch a batch of data and print batch shapes\n",
    "    inputs, targets = next(iter(train_loader))\n",
    "    print(\"\\nBatch Shapes:\")\n",
    "    print(f\"Inputs Shape: {inputs.shape}\")\n",
    "    print(f\"Targets Shape: {targets.shape}\")\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "seq_len = 8\n",
    "config = LMExperimentConfig(\"pytorch_rnn\", seq_len=seq_len, batch_size=batch_size)\n",
    "display_dataloader_sample_with_decoding(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rnn-implementation\"></a>\n",
    "# 2. RNN Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you'll implement a vanilla RNN and LSTM from scratch using PyTorch. You'll then train a character-level language model on a dataset of Shakespeare's writing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rnn\"></a>\n",
    "## 2.1. RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODOs**: Complete the `CustomRNN` class implementation and pass the tests.\n",
    "\n",
    "- **TODO 1: Initialize Parameters**\n",
    "   - Create learnable parameters `Wxh`, `Whh`, and `bh` using `nn.Parameter`\n",
    "   - `Wxh`: Input-to-hidden weights (hidden_size, input_dim)\n",
    "   - `Whh`: Hidden-to-hidden weights (hidden_size, hidden_size) \n",
    "   - `bh`: Hidden bias (hidden_size)\n",
    "   - Makes sure to use exactly these names for the parameters (important for testing)\n",
    "\n",
    "- **TODO 2: Forward Pass**\n",
    "   - Implement the RNN update equation: $h_t = \\text{tanh}(W_{xh} \\cdot x_t + W_{hh} \\cdot h_{t-1} + b_h)$\n",
    "\n",
    "- **TODO 3: Weight Initialization** \n",
    "   - Apply Xavier initialization to `Wxh` and `Whh`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.Wxh = None\n",
    "        self.Whh = None\n",
    "        self.bh = None\n",
    "        # TODO 1: Initialize the weights and biases, the name of the parameters should be Wxh, Whh, and bh (important for testing)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # TODO 3: Apply Xavier initialization (crucial for training!)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x, h_0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, input_dim)\n",
    "            h_0: (1, batch_size, hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            out: (batch_size, seq_len, hidden_size)\n",
    "            h_n: (1, batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        output, h_t = None, None\n",
    "\n",
    "        seq_len = x.shape[1]\n",
    "        h_t = h_0\n",
    "        output = []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :]\n",
    "            # TODO 2: Implement the forward pass of the RNN and append the hidden states to the output list\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "            output.append(h_t.transpose(0, 1))\n",
    "\n",
    "        output = torch.cat(output, dim=1)\n",
    "        return output, h_t\n",
    "\n",
    "\n",
    "TestRNN.test_output_shape(CustomRNN)\n",
    "TestRNN.test_output_equality(CustomRNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lstm\"></a>\n",
    "## 2.2. LSTM (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Implement the missing part of the `CustomLSTM` class using the formula below:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f_t = \\sigma(W_f \\cdot [x_t, h_{t-1}] + b_f) \\\\\n",
    "i_t = \\sigma(W_i \\cdot [x_t, h_{t-1}] + b_i) \\\\\n",
    "\\tilde{C}_t = \\tanh(W_c \\cdot [x_t, h_{t-1}] + b_c) \\\\\n",
    "C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t \\\\\n",
    "O_t = \\sigma(W_o \\cdot [x_t, h_{t-1}] + b_o) \\\\\n",
    "h_t = O_t \\odot \\tanh(C_t) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The formulas are similar to what you've seen in the slides. After implementing the forward pass, we will test 1. the output shape and 2. if it's output values are close to PyTorch's LSTM implementation. If you get stuck, you can still proceed to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # LSTMs have four gates, so map input & hidden state to 4 * hidden_size\n",
    "        self.Wih = nn.Parameter(torch.randn(4 * hidden_size, input_dim))\n",
    "        self.Whh = nn.Parameter(torch.randn(4 * hidden_size, hidden_size))\n",
    "        self.bih = nn.Parameter(torch.zeros(4 * hidden_size))\n",
    "        self.bhh = nn.Parameter(torch.zeros(4 * hidden_size))\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self) -> None:\n",
    "        # Xavier initialization\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for param in self.parameters():\n",
    "            init.uniform_(param, -stdv, stdv)\n",
    "\n",
    "    def forward(self, x, initial_states):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, input_dim)\n",
    "            initial_states: (h_0, c_0)\n",
    "                h_0: (1, batch_size, hidden_size)\n",
    "                c_0: (1, batch_size, hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            out: (batch_size, seq_len, hidden_size)\n",
    "            (h_n, c_n):\n",
    "                h_n: (1, batch_size, hidden_size)\n",
    "                c_n: (1, batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        seq_len = x.shape[1]\n",
    "        h_t, c_t = initial_states\n",
    "        h_t = h_t.squeeze(0)\n",
    "        c_t = c_t.squeeze(0)\n",
    "\n",
    "        hidden_seq = []\n",
    "\n",
    "        # Decompose the parameters into gate-specific weights and biases\n",
    "        W = torch.cat([self.Wih, self.Whh], dim=1)\n",
    "        b = self.bih + self.bhh\n",
    "        Wi, Wf, Wc, Wo = W.chunk(4, dim=0)\n",
    "        bi, bf, bc, bo = b.chunk(4, dim=0)\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :]\n",
    "            # (batch_size, hidden_size + input_dim)\n",
    "            # TODO: Implement the forward pass of the LSTM\n",
    "            hx_cat = torch.cat([x_t, h_t], dim=1)\n",
    "            f_t = torch.sigmoid(hx_cat @ Wf.T + bf)  # (batch_size, hidden_size)\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "            hidden_seq.append(h_t.unsqueeze(0))\n",
    "\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
    "        return hidden_seq, (h_t.unsqueeze(0), c_t.unsqueeze(0))\n",
    "\n",
    "\n",
    "TestLSTM.test_output_shape(CustomLSTM)\n",
    "TestLSTM.test_output_equality(CustomLSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"experiment-lm\"></a>\n",
    "# 3. Experiment: Character-Level Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing the RNN, you'll train a character-level language model on the Shakespeare dataset.\n",
    "\n",
    "**TODO**: Open TensorBoard and run the following cells to train the RNN model.\n",
    "\n",
    "You can use the PyTorch RNN implementation in case you encounter issues with the custom RNN implementation or for comparison.\n",
    "\n",
    "If implemented correctly, the `custom_rnn` model should achieve a validation loss below 1.9 and the `custom_lstm` model should achieve a validation loss below 1.8. Each model should take less than 10 minutes to train on a GPU, or about 15-20 minutes on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_rnn_config = LMExperimentConfig(rnn_type=\"rnn\")\n",
    "model = CustomRNN(lm_rnn_config.vocab_size, lm_rnn_config.hidden_size)\n",
    "# alternatively, you can uncomment the following line to use the PyTorch RNN\n",
    "# model = nn.RNN(lm_rnn_config.vocab_size, lm_rnn_config.hidden_size, batch_first=True)\n",
    "rnn_trainer = LMTrainer(model, lm_rnn_config)\n",
    "rnn_trainer.run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_lstm_config = LMExperimentConfig(rnn_type=\"lstm\")\n",
    "model = CustomLSTM(lm_lstm_config.vocab_size, lm_lstm_config.hidden_size)\n",
    "# alternatively, you can uncomment the following line to use the PyTorch LSTM\n",
    "# model = nn.LSTM(lm_lstm_config.vocab_size, lm_lstm_config.hidden_size, batch_first=True)\n",
    "lstm_trainer = LMTrainer(model, lm_lstm_config)\n",
    "lstm_trainer.run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate some text from the trained model to see how well it captures the style of Shakespeare's writing. Keep in mind that this is a very small model, trained on a tiny dataset. Which aspects of the data did the model learn, and where does it struggle still? If you want to, you may also play around with training for more steps or increasing the model size to see if you can improve on these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_utils.nlp.lm.utils import format_generation_logging, generate_text\n",
    "\n",
    "prompt = \"Now are our brows bound with victorious wreaths;\"\n",
    "\n",
    "# also test with lstm_trainer once you trained it\n",
    "text_gen = generate_text(rnn_trainer.model, rnn_trainer.tokenizer, 1024, prompt)\n",
    "text_gen = format_generation_logging(text_gen, prompt)\n",
    "display(Markdown(text_gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"neural-machine-translation\"></a>\n",
    "# 4. Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Machine Translation (NMT) is the task of translating text from one language to another using neural networks. In this assignment, we will use a [Seq2Seq](https://arxiv.org/abs/1409.3215) model to perform NMT on the [Multi30k](https://arxiv.org/abs/1605.00459) dataset, which contains parallel sentences in English and German (and French). Our goal is to train a transformer model to translate German sentences to English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dataset-preparation\"></a>\n",
    "## 4.1. Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original Multi30k dataset consists of 29,000 training sentences and 1,014 validation sentences. We also augment the dataset with synthetic data generated by translating the original german sentences to english with a pre-trained model, resulting in a total of 60k training sentences.\n",
    "\n",
    "Let's start by loading the dataset and examining a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_utils.nlp.nmt.utils import create_nmt_dataloaders\n",
    "\n",
    "config = NMTExperimentConfig(batch_size=4, rnn_type=\"lstm\")\n",
    "train_loader, _ = create_nmt_dataloaders(config)\n",
    "train_dataset = train_loader.dataset\n",
    "\n",
    "idx = torch.arange(3)\n",
    "src_ids = [train_dataset[i][0] for i in idx]\n",
    "tgt_ids = [train_dataset[i][1] for i in idx]\n",
    "\n",
    "src_sents = config.src_tokenizer.decode_batch(src_ids)\n",
    "tgt_sents = config.tgt_tokenizer.decode_batch(tgt_ids)\n",
    "\n",
    "print(\"Source sentences:\")\n",
    "for sent_en in src_sents:\n",
    "    print(sent_en)\n",
    "\n",
    "print(\"\\nTarget sentences:\")\n",
    "for sent_en in tgt_sents:\n",
    "    print(sent_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tokenization\"></a>\n",
    "## 4.2. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you saw earlier, tokenization is the process of converting text into numerical tokens that can be fed into a neural network. However, while splitting a text into individual characters is possible, it typically results in an impractically large number of tokens and vocabulary size for large datasets.\n",
    "\n",
    "Conversely, splitting text based on spaces to extract whole words also has its drawbacks. For instance, certain phrases (like \"New York\") consist of multiple words, some words might be misspelled, or the text may be in a language that doesn’t use spaces (e.g., Chinese).\n",
    "\n",
    "To address these limitations, subword tokenization methods like Byte Pair Encoding (BPE) and SentencePiece were developed. These techniques can effectively handle such cases. In this assignment, you won't need to implement these tokenization methods from scratch; instead, we’ve provided a wrapper class that utilizes the SentencePiece tokenizer to process the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_de = \"Zwei Männer essen in einer Cafeteria.\"\n",
    "result = config.src_tokenizer.tokenizer.encode(sent_de)\n",
    "print(\"Text:\", sent_de)\n",
    "print(\"Tokens:\", result.tokens)\n",
    "print(\"Token ids:\", result.ids)\n",
    "\n",
    "print(\"-\" * 60)\n",
    "\n",
    "sent_en = \"Two men are eating food in a cafeteria.\"\n",
    "result = config.tgt_tokenizer.tokenizer.encode(sent_en)\n",
    "print(\"Text:\", sent_en)\n",
    "print(\"Tokens:\", result.tokens)\n",
    "print(\"Token ids:\", result.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"batching-and-padding\"></a>\n",
    "## 4.3. Batching and Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we visualize how sentences of varying lengths are padded within a batch using the padding token `<pad>`. This is necessary to ensure uniform length for training.\n",
    "\n",
    "Padding is necessary for batching but introduces complexity, as it requires special handling with attention masks to ignore padded tokens during the attention mechanism. You will have to deal with this in the transformer model implementation.\n",
    "\n",
    "Additionally, each target sentence includes a start token `<s>` at the beginning and an end token `</s>` at the end. These tokens help the model identify the sequence boundaries during training and inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_batch(src_ids, tgt_ids, config):\n",
    "    print(f\"Tensor Shapes: src={src_ids.shape}, tgt={tgt_ids.shape}\")\n",
    "    print(\"Special Tokens:\", config.src_tokenizer.special_tokens, \"\\n\")\n",
    "    src_ids, tgt_ids = src_ids.tolist(), tgt_ids.tolist()\n",
    "\n",
    "    for i, (src, tgt) in enumerate(zip(src_ids, tgt_ids), 1):\n",
    "        src_text = config.src_tokenizer.tokenizer.decode(src, skip_special_tokens=False)\n",
    "        tgt_text = config.tgt_tokenizer.tokenizer.decode(tgt, skip_special_tokens=False)\n",
    "\n",
    "        print(f\"Pair {i}:\")\n",
    "        print(f\"DE: {src_text}\")\n",
    "        print(f\"EN: {tgt_text}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "batch = next(iter(train_loader))\n",
    "src_ids, tgt_ids = batch\n",
    "format_batch(src_ids, tgt_ids, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bleu\"></a>\n",
    "## 4.4. BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[BLEU](https://www.aclweb.org/anthology/P02-1040.pdf) is a metric for evaluating the quality of machine translations. It compares the model's translations (hypotheses) against human reference translations by calculating n-gram overlaps between them. The score ranges from 0 to 100, where higher scores indicate better translation quality. \n",
    "\n",
    "In this assigment, we use the [sacrebleu library](https://github.com/mjpost/sacrebleu?tab=readme-ov-file) to compute corpus-level BLEU scores by comparing the model's predicted translations against the reference translations from our validation dataset.\n",
    "\n",
    "Note that BLEU scores are not perfect and may not always correlate with human judgment. However, they provide a useful quantitative measure for comparing different models and hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"implementation-nmt\"></a>\n",
    "# 5. NMT Implementation: Building Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you'll implement the Encoder and Decoder of the [Seq2Seq](https://arxiv.org/abs/1409.3215) model. You can refer to the Figure 1 in the paper for the model architecture. \n",
    "\n",
    "#### Architecture:\n",
    "1. **Encoder**: The encoder processes the input sequence token by token, producing a context vector that summarizes the input sequence. It typically uses an RNN (like LSTM or GRU) to generate hidden states for each token in the input. The final hidden state of the encoder is used as the initial hidden state for the decoder.\n",
    "\n",
    "2. **Decoder**: The decoder generates the target sequence token by token, using the context vector from the encoder as its initial hidden state. At each time step, the decoder takes the previous token (or an embedding of it) as input and predicts the next token in the sequence. The RNN hidden state is updated at each step based on the previous hidden state and the current input token.\n",
    "\n",
    "#### Operations:\n",
    "1. **Training Process**: During training, the decoder receives the actual target token from the previous time step (teacher forcing). This helps the model learn the correct mapping from the input sequence to the target sequence.\n",
    "\n",
    "2. **Inference (Translation)**: During inference, we feed the source tokens into the encoder to get the context vector. The decoder will receive the context vector and a **start token**, then generate the next tokens autoregressively until it predicts an **end token** or reaches the maximum sequence length. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"encoder\"></a>\n",
    "## 5.1. Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Complete the `Encoder` class implementation and pass the tests. \n",
    "- The input is the source token IDs, and the output is the last encoder hidden states.\n",
    "- `pad_id` is the ID of the padding token.\n",
    "- You should use [pack_padded_sequence](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html) to handle variable-length sequences. \n",
    "- Forward pass should be: \n",
    "    1. Embedding layer\n",
    "    2. Dropout\n",
    "    3. Convert to `PackedSequence`\n",
    "    4. Feed to LSTM and get the output\n",
    "\n",
    "**Hint**: \n",
    "- We have calculated the lengths of the sequences as `source_lengths` for you, which will be used in the `pack_padded_sequence` function.\n",
    "- Use `enforce_sorted=False` and `batch_first=True` in `pack_padded_sequence`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "\n",
    "class NMTEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.embed_size)\n",
    "\n",
    "        assert config.rnn_type in [\"rnn\", \"lstm\"]\n",
    "        rnn_module = nn.LSTM if config.rnn_type == \"lstm\" else nn.RNN\n",
    "\n",
    "        self.rnn = rnn_module(\n",
    "            input_size=config.embed_size,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_layers=config.num_layers,\n",
    "            dropout=config.dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, src, pad_id):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: input tokens ids of shape (batch_size, T)\n",
    "                 where T is the largest sequence length in the batch\n",
    "            pad_id: padding token id\n",
    "\n",
    "        Returns:\n",
    "            last_hidden: last hidden state. Tuple of (hidden, cell)\n",
    "                hidden: (batch_size, num_layers, hidden_size)\n",
    "                cell: (batch_size, num_layers, hidden_size)\n",
    "                where num_layers is the number of LSTM layers\n",
    "        \"\"\"\n",
    "        source_lengths = (src != pad_id).sum(1).tolist()\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return last_hidden\n",
    "\n",
    "\n",
    "TestNMTEncoder.test_output_shape(NMTEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"decoder\"></a>\n",
    "## 5.2. Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Complete the `Decoder` class implementation and pass the tests.\n",
    "- The input `init_hidden` is the initial hidden state of the decoder, which is the last hidden state of the encoder.\n",
    "- Forward pass should be:\n",
    "    1. Embedding layer\n",
    "    2. Dropout\n",
    "    3. LSTM\n",
    "    4. Output projection layer (linear layer)\n",
    "- Note that this time we need to both pack the sentences before feeding them to the LSTM using [pack_padded_sequence](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html), and unpack the output after the LSTM layer using [pad_packed_sequence](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html).\n",
    "\n",
    "**Hint**: \n",
    "- Use `enforce_sorted=False` and `batch_first=True` in `pack_padded_sequence`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "\n",
    "\n",
    "class NMTDecoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.embed_size)\n",
    "\n",
    "        assert config.rnn_type in [\"rnn\", \"lstm\"]\n",
    "        rnn_module = nn.LSTM if config.rnn_type == \"lstm\" else nn.RNN\n",
    "\n",
    "        self.rnn = rnn_module(\n",
    "            input_size=config.embed_size,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_layers=config.num_layers,\n",
    "            dropout=config.dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.vocab_size)\n",
    "\n",
    "    def forward(self, tgt, init_hidden, pad_id):\n",
    "        \"\"\"\n",
    "        This can be used to decode a single step or the whole sequence.\n",
    "\n",
    "        Args:\n",
    "            tgt: input tokens (ids) of shape (batch_size, T)\n",
    "                T can be 1 or more.\n",
    "                When T=1, this is for decode a single step.\n",
    "                Otherwise, this is for decode the whole sequence.\n",
    "            init_hidden: initial hidden state of shape (batch_size, num_layers, hidden_size)\n",
    "            pad_id: padding token id\n",
    "\n",
    "        Returns:\n",
    "            logits: (batch_size, T, vocab_size)\n",
    "            hidden: (batch_size, num_layers, hidden_size)\n",
    "        \"\"\"\n",
    "        target_lengths = (tgt != pad_id).sum(1).tolist()\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return logits, hidden\n",
    "\n",
    "\n",
    "TestNMTDecoder.test_output_shape(NMTDecoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"experiment-nmt\"></a>\n",
    "# 6. Experiment: Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Open TensorBoard and run the following cells to train the Seq2Seq model (RNN and LSTM) on the Multi30k dataset.\n",
    "\n",
    "Default training configurations (10k steps, LSTM) takes around 10-15 minutes on a GPU. If you implemented the model correctly, you should see a validation loss around 2.4, and BLEU score around 22. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.nmt import Seq2Seq\n",
    "\n",
    "config = NMTExperimentConfig(rnn_type=\"lstm\")\n",
    "model = Seq2Seq(NMTEncoder, NMTDecoder, config)\n",
    "trainer = NMTTrainer(model, config)\n",
    "trainer.run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some examples of translations generated by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_translations(trainer):\n",
    "    start_tag = '<div style=\"font-size: 14px; line-height: 1.5;\">\\n'\n",
    "    body = trainer.get_random_examples()\n",
    "    end_tag = \"\\n</div>\"\n",
    "    display(Markdown(start_tag + body + end_tag))\n",
    "\n",
    "\n",
    "display_translations(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"questions\"></a>\n",
    "# 7. Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job on completing the assignment! While we’ve implemented some parts of the code for you, it's important for you to understand the entire process and the reasoning behind it. To wrap things up, take a moment to think about the following questions (you may have to refer to the code to answer them):\n",
    "\n",
    "1. For the character-level language model, the input characters are tokenized into a sequence of integers, but the RNN requires vectors as input. How did we transform the integer sequence into vectors before passing it to the RNN? (`MiniLM` in `models/mini_lm.py`)\n",
    "\n",
    "2. Describe the method we use to generate text from the trained language model. How does the text generation process work? (`MiniLM` in `models/mini_lm.py`)\n",
    "\n",
    "3. Describe the difference between our language model and the translation model in terms of their text sampling strategies. (`MiniLM` in `models/mini_lm.py` and `Seq2Seq` in `models/nmt.py`)\n",
    "\n",
    "4. In `NMTEncoder`, what potential issues might arise if we don't use `pack_padded_sequence` before feeding the input to the LSTM? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"references\"></a>\n",
    "# 8. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Seq2Seq: [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)\n",
    "- BLEU: [BLEU: a Method for Automatic Evaluation of Machine Translation](https://www.aclweb.org/anthology/P02-1040.pdf)\n",
    "- Multi30k: [Multi30K: Multilingual English-German Image Descriptions](https://arxiv.org/abs/1605.00459)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baidl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
