{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BridgingAI Logo](../bridgingai_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Exercise 5: Transformers\n",
    "\n",
    "---\n",
    "1. [Multi-Head Attention](#multi-head-attention)\n",
    "2. [Building GPT](#gpt)\n",
    "3. [Experiment: Language Modeling](#experiment-lm)\n",
    "4. [Encoder-Decoder Transformer](#transformer)\n",
    "<br/> &#9; 4.1 [Transformer Encoder Block](#transformer-encoder-block)\n",
    "<br/> &#9; 4.2 [Transformer Decoder Block](#transformer-decoder-block)\n",
    "\n",
    "5. [Experiment: Neural Machine Translation](#experiment-nmt)\n",
    "6. [Questions](#questions)\n",
    "7. [References](#references)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from configs.nmt_config import NMTExperimentConfig\n",
    "from trainers.nmt_trainer import NMTTrainer\n",
    "from configs.lm_config import LMExperimentConfig\n",
    "from trainers.lm_trainer import LMTrainer\n",
    "from tests.lm_sanity_checks import TestGPTBlock\n",
    "from tests.nmt_sanity_checks import (\n",
    "    TestMultiheadAttention,\n",
    "    TestTransformerEncoderBlock,\n",
    "    TestTransformerDecoderBlock,\n",
    ")\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers have revolutionized the field of Natural Language Processing (NLP) by enabling models to capture long-range dependencies in sequential data through self-attention mechanisms. While most large language models (LLMs) today are built using decoder-only architectures, the original transformer model proposed in [Attention is All You Need](https://arxiv.org/abs/1706.03762) consists of both an encoder and a decoder, a setup which is particularly useful for tasks such as Neural Machine Translation (NMT).\n",
    "\n",
    "In this assignment, you will implement both the gpt-like model to perform language modeling and the full transformer model to perform Neural Machine Translation (NMT).\n",
    "\n",
    "Similar to the previous assignments, we will use Shakespeare's plays as our dataset for the language modeling task and the [Multi30k](https://arxiv.org/abs/1605.00459) dataset for the NMT task. The Multi30k dataset consists of English and German sentence pairs, which we will use to train a transformer model to translate English sentences to German.\n",
    "\n",
    "After completing this assignment, you will be able to:\n",
    "- Implement Multi-Head Attention from scratch.\n",
    "- Build a GPT-like model for language modeling.\n",
    "- Build a full transformer model for Neural Machine Translation.\n",
    "\n",
    "This assignment will mainly focus on the model architecture. If you need a refresher on the data processing and training pipeline, you can refer to the previous RNN assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='multi-head-attention'></a>\n",
    "# 1. Multi-Head Attention from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will implement a `MultiheadAttention` module from scratch using PyTorch. This is one of the core components of Transformer models, where the multi-head attention mechanism allows the model to jointly attend to information from different representation subspaces.\n",
    "\n",
    "The following figure illustrates the attention process you'll be implementing:\n",
    "\n",
    "<img src=\"./assets/attention.svg\" alt=\"Transformer Model\" style=\"width: auto; height: 300px;\">\n",
    "\n",
    "\n",
    "\n",
    "**TODOs**:\n",
    "Complete the `MultiheadAttention` class by filling in specific sections marked as **TODO** (You can skip `TODO 7` for now). Follow the instructions for each step carefully.\n",
    "\n",
    "- **TODO 1: Input Projections**\n",
    "   - Project the `query`, `key`, and `value` inputs using linear layers.\n",
    "   - These projections prepare the data for the attention mechanism.\n",
    "\n",
    "- **TODO 2: Reshape for Multi-Head**\n",
    "   - Split the projected inputs into multiple heads by reshaping.\n",
    "   - Use `self.num_heads` and `self.head_dim` to reshape, then transpose for efficient computation.\n",
    "\n",
    "- **TODO 3: Compute Attention Weights (Part 1)**\n",
    "   - Compute $\\frac{QK^T}{\\sqrt{d}}$\n",
    "\n",
    "- **TODO 4: Compute Attention Weights (Part 2)**\n",
    "   - Compute $\\text{softmax}(\\frac{QK^T}{\\sqrt{d}})$\n",
    "\n",
    "- **TODO 5: Compute Weighted Sum**\n",
    "   - Use the normalized attention weights to compute the weighted sum of values (`v`).\n",
    "   - $\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d}}) V$\n",
    "   - This step forms the core output of the attention mechanism.\n",
    "\n",
    "- **TODO 6: Output Projection**\n",
    "   - Concatenate the heads' outputs and apply a final linear transformation.\n",
    "   - This projects the combined output back to the original embedding dimension.\n",
    "   - If you encounter an error related to `contiguous`, you can use the `contiguous()` function to ensure the tensor is contiguous in memory.\n",
    "\n",
    "\n",
    "The goal here is to make sure you understand the process of multi-head attention thoroughly. If you get stuck, you can refer to the `CausalSelfAttention` class in [minGPT/model.py](https://github.com/karpathy/minGPT/blob/master/mingpt/model.py) from [Andrej Karpathyâ€™s minGPT repository](https://github.com/karpathy/minGPT/tree/master) for inspiration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout, attn_dropout):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: (B, len_q, C)\n",
    "            key: (B, len_k, C)\n",
    "            value: (B, len_k, C)\n",
    "            key_padding_mask: (B, len_k)\n",
    "                Bool tensor where False values are positions that should be masked with -inf.\n",
    "            attn_mask: (len_q, len_k)\n",
    "                Bool tensor where False values are positions that should be masked with -inf.\n",
    "\n",
    "        Returns:\n",
    "            attn_output: (B, len_q, C)\n",
    "            attn_weights: (B, num_heads, len_q, len_k)\n",
    "\n",
    "        B: batch_size.\n",
    "        C: embed_dim.\n",
    "        len_q: length of query (target) sequence\n",
    "        len_k: length of key (source) sequence\n",
    "        \"\"\"\n",
    "        B, len_q, C = query.shape\n",
    "        len_k = key.shape[1]\n",
    "        head_dim = C // self.num_heads\n",
    "\n",
    "        # TODO 1: Input projection\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        assert q.shape == (B, len_q, C)\n",
    "        assert k.shape == (B, len_k, C)\n",
    "        assert v.shape == (B, len_k, C)\n",
    "\n",
    "        # TODO 2: Reshape to multi-head\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        assert q.shape == (B, self.num_heads, len_q, head_dim)\n",
    "        assert k.shape == (B, self.num_heads, len_k, head_dim)\n",
    "        assert v.shape == (B, self.num_heads, len_k, head_dim)\n",
    "\n",
    "        # TODO 3: Compute attention weights\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        assert attn_weights.shape == (B, self.num_heads, len_q, len_k)\n",
    "\n",
    "        # Apply attention mask\n",
    "        if attn_mask is not None:\n",
    "            attn_weights = attn_weights.masked_fill(attn_mask == False, -float(\"inf\"))\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            # TODO 7: Apply key padding mask\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        # TODO 4: Apply softmax\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        assert attn_weights.shape == (B, self.num_heads, len_q, len_k)\n",
    "\n",
    "        # Apply dropout on attention weights\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "\n",
    "        # TODO 5: Compute weighted sum of values as output\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        assert attn_output.shape == (B, self.num_heads, len_q, head_dim)\n",
    "\n",
    "        # TODO 6: Output projection\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        assert attn_output.shape == (B, len_q, C)\n",
    "\n",
    "        # Apply dropout on output\n",
    "        attn_output = self.out_dropout(attn_output)\n",
    "\n",
    "        return attn_output, attn_weights\n",
    "\n",
    "\n",
    "# Sanity check\n",
    "TestMultiheadAttention.test_basic(MultiheadAttention, use_mask=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of masks that will be used by `MultiheadAttention`:\n",
    "1. `attn_mask`: This mask is used to control which query can attend to which key, e.g., the causal mask in the self-attention module in the decoder that prevents tokens from attending to future tokens.\n",
    "2. `key_padding_mask`: This mask is used to ignore padding tokens during the attention mechanism.\n",
    "\n",
    "**TODO 7**: \n",
    "\n",
    "Complete `key_padding_mask` part in the `forward` method. \n",
    "\n",
    "**Hints**:\n",
    "- You can use the `masked_fill` function to apply the mask to the attention weights (similar to how we handle the `attn_mask`).\n",
    "- You can refer to the docstring to understand the shape and expected behavior of the `key_padding_mask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestMultiheadAttention.test_basic(MultiheadAttention, use_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestMultiheadAttention.test_specific_mask(MultiheadAttention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gpt'></a>\n",
    "# 2. Building GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will implement the `GPTBlock` class, the main building block of GPT-like models, which use a decoder-only Transformer architecture for language modeling. These models predict the next token in a sequence based on the previous tokens, using self-attention and MLPs.\n",
    "\n",
    "#### Key Features of the `GPTBlock`:\n",
    "1. **Self-Attention Only**: Unlike full Transformer models, GPT models do not use cross-attention. The `GPTBlock` only utilizes (masked) self-attention.\n",
    "2. **Causal Masking**: To ensure that each token can only attend to previous tokens and not future ones, the self-attention mechanism applies a causal mask.\n",
    "3. **Pre-Norm Architecture**: This block follows the \"pre-norm\" configuration, where layer normalization is applied before the self-attention and feed-forward layers. This approach is based on the findings in [On Layer Normalization in the Transformer Architecture](https://arxiv.org/abs/2002.04745) (Figure 1(b)).\n",
    "\n",
    "#### GPTBlock Architecture:\n",
    "Below is an illustration of the architecture of the `GPTBlock`:\n",
    "\n",
    "<img src=\"./assets/gpt.svg\" alt=\"GPT Model Architecture\" style=\"width: auto; height: 300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODOs**: Complete the `GPTBlock` class implementation using the architecture described above.\n",
    "\n",
    "- **TODO 1: Self-Attention**\n",
    "   - Remember to use a causal mask.\n",
    "   - The `causal_mask` buffer can be utilized as causal mask. \n",
    "\n",
    "- **TODO 2: Feed-Forward**\n",
    "   - Implement the feed-forward part of the block using the class members.\n",
    "\n",
    "**Hints**:\n",
    "- You need to resize the buffer to the correct shape for the current input as the input sequence length may vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.modules import FeedForwardBlock\n",
    "\n",
    "\n",
    "class GPTBlock(nn.Module):\n",
    "    def __init__(self, transformer_config):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(transformer_config.n_embd)\n",
    "        self.norm2 = nn.LayerNorm(transformer_config.n_embd)\n",
    "\n",
    "        self.attn = MultiheadAttention(\n",
    "            transformer_config.n_embd,\n",
    "            transformer_config.n_head,\n",
    "            transformer_config.dropout,\n",
    "            transformer_config.attn_dropout,\n",
    "        )\n",
    "        # This is a simple two-layer MLP\n",
    "        self.ff = FeedForwardBlock(\n",
    "            transformer_config.n_embd, transformer_config.dropout\n",
    "        )\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"causal_mask\",\n",
    "            torch.tril(\n",
    "                torch.ones(\n",
    "                    transformer_config.context_length, transformer_config.context_length\n",
    "                )\n",
    "            ).to(torch.bool),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, T, C)\n",
    "                B: batch size\n",
    "                T: sequence length\n",
    "                C: number of channels\n",
    "\n",
    "        Returns:\n",
    "            output tensor of shape (B, T, C)\n",
    "        \"\"\"\n",
    "        _, T, _ = x.shape\n",
    "\n",
    "        # TODO 1: Self-attention\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # TODO 2: Feed-forward\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "TestGPTBlock.test_basic(GPTBlock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the GPTBlock, we can now build a complete Transformer Decoder for causal language modeling. The overall model architecture is quite simple - most of the functionality is bundled in the GPTBlock!\n",
    "\n",
    "During inference, the Transformer Decoder works similar to an RNN by iteratively applying the model to the sequence to predict the next token (take a look at the `generate` method). Training, however, is parallelized over the entire sequence length, making it much easier to scale to large model sizes.\n",
    "\n",
    "**TODO**: complete the `compute_loss` function. You will need to apply the model to the input data and then compute the cross-entropy loss between the logits and the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.modules import Embedding\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        config = config.transformer_config\n",
    "        self.config = config\n",
    "        self.embedding = Embedding(\n",
    "            config.vocab_size, config.context_length, config.n_embd, config.dropout\n",
    "        )\n",
    "\n",
    "        self.blocks = nn.ModuleList([GPTBlock(config) for _ in range(config.n_layer)])\n",
    "        self.norm = nn.LayerNorm(config.n_embd)\n",
    "        self.out_proj = nn.Linear(config.n_embd, config.vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        x = self.embedding(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "    def compute_loss(self, input_data, target_data):\n",
    "        # TODO: apply the model to the inputs and compute the cross-entropy loss\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(model, context_ids, max_new_tokens: int = 500, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Generate text using the model with proper temperature scaling\n",
    "\n",
    "        Args:\n",
    "            context_ids: tokens indices of shape (T, )\n",
    "            max_new_tokens: maximum number of tokens to generate\n",
    "            temperature: controls randomness (higher = more random, lower = more deterministic)\n",
    "        \"\"\"\n",
    "        was_training = model.training\n",
    "        model.eval()\n",
    "        device = next(model.parameters()).device\n",
    "\n",
    "        T = context_ids.shape[0]\n",
    "        context_ids = context_ids.view(1, -1).clone().to(device)\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Get the last context_length tokens\n",
    "            x = context_ids[:, -model.config.context_length :]\n",
    "\n",
    "            # Get logits and apply temperature\n",
    "            logits = model(x)[:, -1, :]\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax and sample\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            context_ids = torch.cat([context_ids, next_token], dim=-1)\n",
    "\n",
    "        if was_training:\n",
    "            model.train(was_training)\n",
    "        return context_ids.squeeze()[T:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='experiment-lm'></a>\n",
    "# 3. Experiment: Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have implemented the `MultiheadAttention` and `GPTBlock` classes. Now, you will use these components to build a GPT-like model for language modeling.\n",
    "\n",
    "**TODO**: \n",
    "1. **Start TensorBoard**: Monitor the training progress and metrics. We also log text samples periodically (under 'text').\n",
    "2. **Run the Training Script**: Execute the cell below to begin training.\n",
    "\n",
    "**Notes**: \n",
    "- You can review the hyperparameters used for training in `configs/lm_config.py`.\n",
    "- To see alternative Transformer configurations, check `configs/transformer_config.py`.\n",
    "\n",
    "The default training configuration (`gpt-nano`, `max_steps=10000`) takes around 15 minutes to train on a GPU. If implemented correctly, you should see the validation loss under 1.7 and train loss under 1.6 by the end of training. What do you observe when you train a larger model (`gpt-micro`, `gpt-mini`)? Do you see differences in the training/validation curves or the generated text?\n",
    "\n",
    "**Hint:** When trying out larger model configs, you can save some time by running validation less often since it quickly becomes quite computationally expensive (remember that generating 1000 tokens requires 1000 forward passes!). To do this, adapt the `eval_every_n_steps` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_config = LMExperimentConfig(\"gpt-nano\")\n",
    "lm_model = GPT(lm_config)\n",
    "lm_trainer = LMTrainer(lm_model, lm_config)\n",
    "lm_trainer.run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the training finished, run the cell below to inspect the text that the model generates. Keep in mind that this is an extremely small model trained on a tiny dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_utils.nlp.lm.utils import format_generation_logging, generate_text\n",
    "\n",
    "prompt = \"Now are our brows bound with victorious wreaths;\"\n",
    "\n",
    "# also test with lstm_trainer once you trained it\n",
    "text_gen = generate_text(lm_trainer.model, lm_trainer.tokenizer, 1024, prompt)\n",
    "text_gen = format_generation_logging(text_gen, prompt)\n",
    "display(Markdown(text_gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transformer'></a>\n",
    "# 4. Encoder-Decoder Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will implement the **encoder-decoder** variant of the Transformer model, similar to the model described in [Attention is All You Need](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "#### Key Components:\n",
    "- **Encoder Block**:\n",
    "  - Multi-Head Self-Attention\n",
    "  - Feed-Forward Network\n",
    "- **Decoder Block**:\n",
    "  - Multi-Head Self-Attention\n",
    "  - Cross-Attention (attends to encoder output)\n",
    "  - Feed-Forward Network\n",
    "\n",
    "We use the **pre-norm architecture**, applying layer normalization before attention and feed-forward layers.\n",
    "\n",
    "#### Transformer Architecture:\n",
    "\n",
    "<img src=\"./assets/prenorm_transformer.svg\" alt=\"Transformer Model\" style=\"width: auto; height: 400px;\">\n",
    "\n",
    "(Note that the figure omitted embeddings, output projection and softmax for clarity.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transformer-encoder-block'></a>\n",
    "## 4.1 Transformer Encoder Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will implement the `TransformerEncoderBlock`, the part of the transformer architecture that processes the source (input) tokens. \n",
    "\n",
    "**TODOs**: Complete the `TransformerEncoderBlock` class implementation using the architecture described above.\n",
    "\n",
    "- **TODO 1: Self-Attention**\n",
    "   - Remember to use `src_key_padding_mask` to prevent the encoder from attending to padded tokens.\n",
    "\n",
    "- **TODO 2: Feed-Forward**\n",
    "   - Implement the feed-forward part of the block using the class members.\n",
    "\n",
    "**Hints**:\n",
    "- Unlike the decoder block, the encoder block doesn't require a `attn_mask` since it doesn't need to prevent tokens from attending to other tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.modules import FeedForwardBlock\n",
    "from models.transformer import create_key_padding_mask\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, transformer_config):\n",
    "        super().__init__()\n",
    "        n_head = transformer_config.n_head\n",
    "        n_embd = transformer_config.n_embd\n",
    "        dropout = transformer_config.dropout\n",
    "        attn_dropout = transformer_config.attn_dropout\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(n_embd)\n",
    "        self.norm2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "        self.self_attn = MultiheadAttention(n_embd, n_head, dropout, attn_dropout)\n",
    "        self.ff = FeedForwardBlock(n_embd, dropout)\n",
    "\n",
    "    def forward(self, src, src_len):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: source sentence embeddings of shape (B, T, C)\n",
    "            src_len: int tensor stating the length of each source sentence in the batch. shape (B,)\n",
    "\n",
    "        Returns:\n",
    "            output of the transformer encoder block. shape (B, T, C)\n",
    "        \"\"\"\n",
    "        # Source key padding mask\n",
    "        src_key_padding_mask = create_key_padding_mask(src_len, src)\n",
    "\n",
    "        # TODO 1: Self-attention\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # TODO 2: Feed-forward\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "TestTransformerEncoderBlock.test_basic(TransformerEncoderBlock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transformer-decoder-block'></a>\n",
    "## 4.2 Transformer Decoder Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will implement the `TransformerDecoderBlock`, which processes the output of the encoder (usually referred to as `memory`) and the tokens of the target sequence. This block combines self-attention, cross attention and feed-forward layers using the architecture described above.\n",
    "\n",
    "**TODOs**: Complete the `TransformerDecoderBlock` class implementation using the pre-norm architecture.\n",
    "\n",
    "- **TODO 1: Self-Attention**\n",
    "   - `key_padding_mask` in the `MultiheadAttention` module should be used to prevent the decoder from attending to padded tokens.\n",
    "   - `attn_mask` should also be specified. This should be a causal mask to prevent tokens from attending to future tokens.\n",
    "\n",
    "- **TODO 2: Cross-Attention**\n",
    "   - This will perform cross-attention between the target tokens and the memory (output of the encoder), where queries are the target tokens and keys and values are the encoder outputs.\n",
    "   - `key_padding_mask` should be used, while `attn_mask` is not required.\n",
    "\n",
    "- **TODO 3: Feed-Forward**\n",
    "   - Implement the feed-forward part of the block using the class members. \n",
    "\n",
    "**Hints**:\n",
    "- The `causal_mask` buffer can be used as the `attn_mask` for the self-attention mechanism in the decoder block. Note that the target sentence lengths may vary between batches, so you need to adjust the size of the `causal_mask` accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, transformer_config):\n",
    "        super().__init__()\n",
    "        n_head = transformer_config.n_head\n",
    "        n_embd = transformer_config.n_embd\n",
    "        dropout = transformer_config.dropout\n",
    "        attn_dropout = transformer_config.attn_dropout\n",
    "        context_length = transformer_config.context_length\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(n_embd)\n",
    "        self.norm2 = nn.LayerNorm(n_embd)\n",
    "        self.norm3 = nn.LayerNorm(n_embd)\n",
    "\n",
    "        self.self_attn = MultiheadAttention(n_embd, n_head, dropout, attn_dropout)\n",
    "        self.cross_attn = MultiheadAttention(n_embd, n_head, dropout, attn_dropout)\n",
    "\n",
    "        self.ff = FeedForwardBlock(n_embd, dropout)\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"causal_mask\",\n",
    "            torch.tril(torch.ones(context_length, context_length)).to(torch.bool),\n",
    "        )\n",
    "\n",
    "    def forward(self, tgt, tgt_len, memory, memory_len):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tgt: target sentence embeddings of shape (B, T, C)\n",
    "            tgt_len: int tensor stating the length of each target sentence in the batch. shape (B,)\n",
    "            memory: source sentence embeddings of shape (B, S, C)\n",
    "            memory_len: int tensor stating the length of each source sentence in the batch. shape (B,)\n",
    "\n",
    "        Returns:\n",
    "            output of the transformer decoder block. shape (B, T, C)\n",
    "\n",
    "        B: batch size\n",
    "        T: target sequence length\n",
    "        S: source sequence length\n",
    "        C: embedding dimension\n",
    "        \"\"\"\n",
    "        _, T, _ = tgt.shape\n",
    "\n",
    "        # create key padding masks\n",
    "        tgt_key_padding_mask = create_key_padding_mask(tgt_len, tgt)\n",
    "        memory_key_padding_mask = create_key_padding_mask(memory_len, memory)\n",
    "\n",
    "        # TODO 1: Self-attention\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # TODO 2: Cross-attention\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # TODO 3: Feed-forward\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "TestTransformerDecoderBlock.test_basic(TransformerDecoderBlock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='experiment-nmt'></a>\n",
    "# 5. Experiment: Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the key components of the Transformer model implemented, you can now proceed to train it on the **Multi30k dataset** and evaluate its performance.\n",
    "\n",
    "**TODO**: \n",
    "1. **Start TensorBoard**: Monitor the training progress and metrics.\n",
    "2. **Run the Training Script**: Execute the cell below to begin training.\n",
    "\n",
    "**Notes**: \n",
    "- You can review the hyperparameters used for training in `configs/nmt_config.py`.\n",
    "- To see alternative Transformer configurations, check `configs/transformer_config.py`.\n",
    "\n",
    "The default training configuration (`gpt-micro`, `max_steps=40000`) takes within 30 minutes to train on a GPU. If you implement the model correctly, you should get a BLEU score somewhere between 33-35 and a validation loss under 1.8.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.transformer import Transformer\n",
    "\n",
    "nmt_config = NMTExperimentConfig(\"gpt-micro\")\n",
    "nmt_model = Transformer(TransformerEncoderBlock, TransformerDecoderBlock, nmt_config)\n",
    "nmt_trainer = NMTTrainer(nmt_model, nmt_config)\n",
    "nmt_trainer.run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to see some example translations generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_translations(trainer):\n",
    "    start_tag = '<div style=\"font-size: 14px; line-height: 1.5;\">\\n'\n",
    "    body = trainer.get_random_examples()\n",
    "    end_tag = \"\\n</div>\"\n",
    "    display(Markdown(start_tag + body + end_tag))\n",
    "\n",
    "\n",
    "display_translations(nmt_trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='questions'></a>\n",
    "# 6. Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the model is being trained, feel free to think about these questions to check your understanding of the transformer architecture.\n",
    "\n",
    "1. How does our positional embedding implementation differ from the [original transformer paper](https://arxiv.org/abs/1706.03762)?\n",
    "2. How does the `key_padding_mask` differ from the `attn_mask` in the transformer model?\n",
    "3. For our machine translation task, would increasing the `context_length` to 2048 or 4096 likely improve the model's performance? Why or why not?\n",
    "4. If the `context_length` is set to 20, how should we process the input data of length 10 before feeding it to the transformer model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='references'></a>\n",
    "# 7. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "- [On Layer Normalization in the Transformer Architecture](https://arxiv.org/abs/2002.04745)\n",
    "- [Multi30K: Multilingual English-German Image Descriptions](https://arxiv.org/abs/1605.00459)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baidl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
