{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BridgingAI Logo](../bridgingai_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Exercise 2: Practical Deep Learning \n",
    "1. [Initialization](#chp1-initialization)\n",
    "2. [Batch Normalization](#chp2-batch-normalization)\n",
    "3. [Layer Normalization](#chp3-layer-normalization)\n",
    "4. [Dropout](#chp4-dropout)\n",
    "5. [Optimization](#chp5-optimization)\n",
    "6. [Experiments](#chp6-experiments)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import math\n",
    "from dataclasses import replace\n",
    "from pprint import pprint\n",
    "from typing import Union, List, Iterable\n",
    "\n",
    "from sanity_checks import run_tests\n",
    "from training import ExperimentConfig, Trainer, create_dataloader\n",
    "from helpers import analyze_activations_combined, InitMethods, train_init_methods, train_model, evaluate_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"chp1-initialization\"></a>\n",
    "# 1. Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proper initialization is a crucial step when training deep neural networks. A bad initialization will have detrimental impact on the convergence of the network.\n",
    "\n",
    "#### Example: Xavier Initialization\n",
    "\n",
    "[Xavier initialization](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf), also known as Glorot initialization, is designed to keep the scale of gradients roughly the same in all layers, _when using tanh activations_. To this end, we need to initialize the weights with a variance of $\\frac{2}{n_{\\text{in}} + n_{\\text{out}}}$, where $n_{in}$ is the number of input neurons and $n_{out}$ is the number of output neurons. We may choose between a uniform or a Gaussian distribution, as long as the variance is correct.\n",
    "\n",
    "Uniform:\n",
    "$W \\sim U(-\\sqrt{\\frac{6}{n_{in} + n_{out}}}, \\sqrt{\\frac{6}{n_{in} + n_{out}}})$\n",
    "\n",
    "Gaussian:\n",
    "$W \\sim N(0, \\frac{2}{n_{\\text{in}} + n_{\\text{out}}})$\n",
    "\n",
    "For more initialization methods (e.g. for different activation functions), refer to the documentation on the [torch.nn.init](https://pytorch.org/docs/stable/nn.init.html) module.\n",
    "\n",
    "**TODO**: Implement Xavier initialization in `custom_xavier_init` function and run the cell to see a visualization of the activation values in the first forward pass.\n",
    "\n",
    "**HINT**: \n",
    "1. To create a uniform distribution between 0 and 1, you can use `torch.rand(...)`.\n",
    "2. We also provide the offical PyTorch implementation of Xavier initialization for visualization. If implemented correctly, the two methods should have similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(784, 256),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def custom_xavier_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        in_features = m.in_features  # int\n",
    "        out_features = m.out_features  # int\n",
    "        weight = m.weight.data  # torch.Tensor\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        m.weight.data = weight\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "init_methods = {\n",
    "    \"Xavier (Yours)\": custom_xavier_init,\n",
    "    \"Xavier (PyTorch)\": InitMethods.xavier_init,\n",
    "    \"Xavier x0.1\": lambda m: InitMethods.scaled_xavier_init(m, 0.1),\n",
    "    \"Xavier x10\": lambda m: InitMethods.scaled_xavier_init(m, 10),\n",
    "    \"Zero\": InitMethods.zero_init,\n",
    "    \"Unit Gaussian\": InitMethods.unit_gaussian_init,\n",
    "}\n",
    "\n",
    "train_loader, test_loader = create_dataloader(ExperimentConfig(\"\", \"\"))\n",
    "analyze_activations_combined(init_methods, SimpleNet, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train a simple neural network using various initialization methods to compare their performance. Some methods are intentionally flawed, resulting in stalled loss or high initial loss. Which models perform well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_init_methods(SimpleNet, init_methods, train_loader, test_loader, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"chp2-batch-normalization\"></a>\n",
    "# 2. Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization normalizes the input of each layer in order to improve the training speed and stability of the neural network. It transforms the input to have a mean of 0 and a standard deviation of 1. This helps to prevent the input from becoming too large or too small, which can cause the network to become unstable. In this part, you will implement the batch normalization layer in PyTorch.\n",
    "\n",
    "**TODO**: Implement the `forward` method in `BatchNorm` class below.\n",
    "\n",
    "**HINT**: \n",
    "1. This layer is similar to the `nn.BatchNorm1d` layer in PyTorch. You can refer to the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html) for information like the parameters and the formula used. One difference is that our implentation only takes 2D input tensors, while the PyTorch implementation can take 2D and 3D.\n",
    "2. After reading the documentation, take a look at the `__init__` method to see what parameters are created. You will also need to call the `update_stats` method in the `forward` method to update the running mean and variance.\n",
    "3. Since the module behaves differently during training and evaluation, you will need to check if the module is in training mode or not. You can use the `self.training` flag to check this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-05, momentum=0.1, affine=True):\n",
    "        \"\"\"\n",
    "        Batch Normalization layer for data shape (N, C)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.affine = affine\n",
    "\n",
    "        # always track running statistics for inference\n",
    "        self.running_mean = torch.zeros(self.num_features)\n",
    "        self.running_var = torch.ones(self.num_features)\n",
    "\n",
    "        self.gamma = None\n",
    "        self.beta = None\n",
    "        if self.affine is True:\n",
    "            self.gamma = nn.Parameter(torch.ones(self.num_features))\n",
    "            self.beta = nn.Parameter(torch.zeros(self.num_features))\n",
    "\n",
    "    def update_stats(self, batch_mean, batch_var):\n",
    "        assert batch_mean.shape == self.running_mean.shape\n",
    "        assert batch_var.shape == self.running_var.shape\n",
    "\n",
    "        self.running_mean = (\n",
    "            1 - self.momentum\n",
    "        ) * self.running_mean + self.momentum * batch_mean\n",
    "        self.running_var = (\n",
    "            1 - self.momentum\n",
    "        ) * self.running_var + self.momentum * batch_var\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x: input tensor of shape (N, C)\n",
    "        Returns:\n",
    "            x: output tensor of shape (N, C)\n",
    "        \"\"\"\n",
    "        assert x.ndim == 2 and x.shape[1] == self.num_features and x.shape[0] > 1\n",
    "        if self.training:\n",
    "            # Train mode\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "\n",
    "            if self.affine:\n",
    "                x = x * self.gamma + self.beta\n",
    "        else:\n",
    "            # Test mode\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "\n",
    "            if self.affine:\n",
    "                x = x * self.gamma + self.beta\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the sanity check\n",
    "run_tests(BatchNorm=BatchNorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Networks with normalization layers usually converge faster than those without. To test this, we will train a simple network with and without batch normalization layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Flatten(1), nn.Linear(784, 128), nn.Tanh(), nn.Linear(128, 10))\n",
    "train_model(model, train_loader, epochs=1)\n",
    "acc = evaluate_model(model, test_loader)\n",
    "print(f\"Accuracy w/o BatchNorm: {acc:.4f}\")\n",
    "\n",
    "print()\n",
    "# If your implementation does not work, uncomment the following line\n",
    "# BatchNorm = nn.BatchNorm1d\n",
    "bn_model = nn.Sequential(nn.Flatten(1), nn.Linear(784, 128), nn.Tanh(), BatchNorm(128), nn.Linear(128, 10))\n",
    "train_model(bn_model, train_loader, epochs=1)\n",
    "acc = evaluate_model(bn_model, test_loader)\n",
    "print(f\"Accuracy w/  BatchNorm: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"chp3-layer-normalization\"></a>\n",
    "# 3. Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although batch normalization is widely used in deep learning, it has some limitations. For example, it can be sensitive to the batch size and have to keep track of the running mean and variance during training. Layer normalization is an alternative normalization technique that normalizes the input of each layer across the features, rather than across the batch dimension. In this part, you will have to implement the layer normalization layer in PyTorch.\n",
    "\n",
    "**TODO**: Implement the `forward` method in `LayerNorm` class below.\n",
    "\n",
    "**HINT**:\n",
    "1. This layer is similar to the `nn.LayerNorm` layer in PyTorch. You can refer to the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) for information like the parameters and the formula used.\n",
    "2. After reading the documentation, take a look at the `__init__` method to see what parameters are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-05, elementwise_affine=True):\n",
    "        \"\"\"\n",
    "        Layer Normalization layer\n",
    "\n",
    "        Inputs:\n",
    "        - normalized_shape: (int or list or torch.Size): input shape from an expected input of size (*, normalized_shape[0], normalized_shape[1], ..., normalized_shape[-1])\n",
    "        - eps (float): a value added to the denominator for numerical stability. Default: 1e-5\n",
    "        - elementwise_affine (bool): a boolean value that when set to True, this module has learnable per-element affine parameters. Default: True\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if isinstance(normalized_shape, int):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "\n",
    "        self.normalized_shape = torch.Size(normalized_shape)\n",
    "        self.eps = eps\n",
    "        self.elementwise_affine = elementwise_affine\n",
    "\n",
    "        if self.elementwise_affine:\n",
    "            self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "            self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        else:\n",
    "            self.register_parameter(\"weight\", None)\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        x: input tensor of shape (*, normalized_shape[0], normalized_shape[1], ..., normalized_shape[-1])\n",
    "\n",
    "        Returns:\n",
    "        output tensor of shape (*, normalized_shape[0], normalized_shape[1], ..., normalized_shape[-1])\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_tests(LayerNorm=LayerNorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layernorm often achieves similar or better results than BatchNorm, and it is less sensitive to the batch size. Run the following code to check the accuracy of the model with LayerNorm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_model = nn.Sequential(nn.Flatten(1), nn.Linear(784, 128), nn.Tanh(), LayerNorm(128), nn.Linear(128, 10))\n",
    "train_model(bn_model, train_loader, epochs=1)\n",
    "acc = evaluate_model(bn_model, test_loader)\n",
    "print(f\"Accuracy w/ LayerNorm: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"chp4-dropout\"></a>\n",
    "# 4. Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Dropout](https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf) is a regularization technique used to prevent overfitting in neural networks. It works by randomly setting a fraction of the input units to zero during training. This helps to prevent the network from becoming too dependent on any one input unit, and encourages the network to learn more robust features. In this part, you will implement the dropout layer in PyTorch.\n",
    "\n",
    "**Vanilla Dropout Algorithm:**\n",
    "1. During training, for each input x:\n",
    "   y = x * mask, where mask ~ Bernoulli(1 - p)\n",
    "2. During inference:\n",
    "   y = (1 - p) * x\n",
    "\n",
    "**Inverted Dropout Algorithm:**\n",
    "1. During training, for each input x:\n",
    "   y = (x * mask) / (1 - p), where mask ~ Bernoulli(1 - p)\n",
    "2. During inference:\n",
    "   y = x\n",
    "\n",
    "The rescaling of activations is necessary to maintain the expected variance of activations. The original and inverted formulation are mathematically equivalent, but inverted dropout has the benefit of simplified inference.\n",
    "\n",
    "**TODO**: Implement the `forward` method in the `Dropout` class below. Please implement **inverted dropout**, where scaling happens during training instead of testing. This is a common approach in modern deep learning frameworks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x: input tensor of shape (N, C)\n",
    "        Returns:\n",
    "            x: output tensor of shape (N, C)\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            # Train mode\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the sanity check\n",
    "run_tests(Dropout=Dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"chp5-optimization\"></a>\n",
    "# 5. Optimizers\n",
    "There are many different optimization algorithms that can be used to train a neural network, such as stochastic gradient descent (SGD), RMSprop, Adam, AdamW, etc. In this exercise, we will compare two popular optimizers: SGD with momentum and the AdamW optimizer. Both have proven to work well in a large variety of different settings; in particular AdamW is a sensible default choice, yielding good performance with little hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 SGD with momentum and weight decay\n",
    "\n",
    "The SGD optimizer with momentum and L2 regularization updates the parameters $\\mathbf{w}$ of the model using the following formulas:\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_t = \\beta \\mathbf{v}_{t-1} + \\left( \\cfrac{\\partial \\text{E}}{\\partial \\mathbf{w}_{t-1}} + \\lambda \\mathbf{w}_{t-1} \\right)\n",
    "$$\n",
    "$$\n",
    "\\mathbf{w}_t = \\mathbf{w}_{t-1} - \\eta \\mathbf{v}_t\n",
    "$$\n",
    "where $\\mathbf{w}_t$ is the updated parameter, $\\mathbf{w}_{t-1}$ is the previous parameter, $\\eta$ is the learning rate, $\\beta$ is the momentum, $\\cfrac{\\partial \\text{E}}{\\partial \\mathbf{w}_{t-1}}$ is the gradient of the loss function with respect to the parameter, and $\\lambda$ is the regularization strength (L2 penalty coefficient).\n",
    "\n",
    "**Notes:**\n",
    "- $\\beta = 0$ yields the regular SGD update (no momentum).\n",
    "- L2 regularization is equivalent to weight decay _only without momentum_, that is in case $\\beta = 0$. When using momentum (or other optimizers), L2 regularization and weight decay are not equivalent. However, this distinction is not always made in the literature and popular frameworks, so be wary!\n",
    "- Weight decay downscales all weights by a small factor in each step (hence the name). This is not appropriate for all parameters in a network! In particular, avoid penalizing biases and affine parameters of normalization layers.\n",
    "\n",
    "Have a good look at the following example implementation of SGD with momentum and weight decay.\n",
    "PyTorch's optimizer base class is very flexible - we're using only a small part of it. For a more comprehensive overview, check the [documentation](https://pytorch.org/docs/stable/optim.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr: float, momentum: float = 0.0, weight_decay: float = 0.0):\n",
    "        # each parameter group is a dictionary containing the parameters and hyperparameters\n",
    "        defaults = dict(lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "        torch.optim.SGD\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        # self.param_groups is a list of dictionaries\n",
    "        for group in self.param_groups:\n",
    "            for param in group['params']:\n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "                    \n",
    "                grad = param.grad\n",
    "                # the optimizer base class manages the state for each parameter\n",
    "                state = self.state[param]\n",
    "                \n",
    "                # Add weight decay to gradient\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad = grad + group['weight_decay'] * param\n",
    "                \n",
    "                # Apply momentum\n",
    "                if group['momentum'] != 0:\n",
    "                    if 'momentum_buffer' not in state:\n",
    "                        state['momentum_buffer'] = torch.clone(grad).detach()\n",
    "                    else:\n",
    "                        # suffix _ means inplace operation\n",
    "                        state['momentum_buffer'].mul_(group['momentum']).add_(grad)\n",
    "                        # equivalent to m = beta * m + grad\n",
    "                    grad = state['momentum_buffer']\n",
    "                \n",
    "                # SGD update\n",
    "                param.add_(grad, alpha=-group['lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 AdamW\n",
    "\n",
    "AdamW combines Adam's adaptive learning rates with decoupled weight decay. For each parameter vector $\\mathbf{w}$, it maintains exponential moving averages of gradients ($\\mathbf{m}$) and squared gradients ($\\mathbf{v}$):\n",
    "\n",
    "**Moment Updates:**\n",
    "$$\\mathbf{m}_t = \\beta_1 \\mathbf{m}_{t-1} + (1-\\beta_1) \\mathbf{g}_t$$\n",
    "$$\\mathbf{v}_t = \\beta_2 \\mathbf{v}_{t-1} + (1-\\beta_2) \\mathbf{g}_t^2$$\n",
    "\n",
    "**Bias Correction:**\n",
    "$$\\hat{\\mathbf{m}}_t = \\mathbf{m}_t/(1-\\beta_1^t)$$\n",
    "$$\\hat{\\mathbf{v}}_t = \\mathbf{v}_t/(1-\\beta_2^t)$$\n",
    "\n",
    "**Parameter Update:**\n",
    "$$\\mathbf{w}_t = (1-\\eta\\lambda)\\mathbf{w}_{t-1} - \\eta\\frac{\\hat{\\mathbf{m}}_t}{(\\sqrt{\\hat{\\mathbf{v}}_t}+\\epsilon)}$$\n",
    "\n",
    "where $\\beta_1=0.9$, $\\beta_2=0.999$ are the decay rates, $\\epsilon$ is a small constant for numerical stability, and $\\lambda$ is the weight decay coefficient. The decoupled weight decay term $(1-\\eta\\lambda)\\mathbf{w}_{t-1}$ distinguishes AdamW from standard Adam. Note that with this formulation, a learning rate scheduler also influences the weight decay rate!\n",
    "\n",
    "**TODO:** implement the update equations of the AdamW optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamW(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            beta1, beta2 = group['betas']\n",
    "            \n",
    "            for param in group['params']:\n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "                    \n",
    "                grad = param.grad\n",
    "                state = self.state[param]\n",
    "                \n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(param)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(param)\n",
    "                \n",
    "                state['step'] += 1\n",
    "                \n",
    "                # Decay the first and second moment running average coefficient\n",
    "                # YOUR CODE HERE\n",
    "                raise NotImplementedError()\n",
    "                \n",
    "                # Bias correction - cheaper to precompute it\n",
    "                # YOUR CODE HERE\n",
    "                raise NotImplementedError()\n",
    "                \n",
    "                # AdamW = Adam + decoupled weight decay\n",
    "                # YOUR CODE HERE\n",
    "                raise NotImplementedError()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the sanity check\n",
    "run_tests(optimizer=AdamW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"chp6-experiments\"></a>\n",
    "# 6. Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will conduct a series of experiments to evaluate the performance of the neural network with/without layer normalization, dropout, and different optimization algorithms. We will use the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset, which has the same size and numbers of images as the original MNIST dataset, but with more challenging images of clothing items.\n",
    "\n",
    "**All experiments log data via TensorBoard.**\n",
    "\n",
    "Open TensorBoard by running the following command in the terminal:\n",
    "```\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "Run the code below to train the neural network with different configurations and analyze the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 SGD vs AdamW\n",
    "In this section, we will compare the performance of the SGD and AdamW optimizers on the Fashion MNIST dataset by training a simple neural network.\n",
    "\n",
    "After training the models, what do you observe when comparing the `train_loss` curves? Which of the two gives a better accuracy, and which one converges faster? Can you tune them to give better results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_config = ExperimentConfig(\n",
    "    exp_name=\"Optimizer\",\n",
    "    config_name=None,\n",
    "    use_layernorm=False,\n",
    "    dropout_rate=0,\n",
    "    max_steps=3000,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "sgd_config = replace(\n",
    "    optimizer_config,\n",
    "    config_name=\"SGD\",\n",
    "    optimizer=\"sgd\",\n",
    "    lr=1e-2,\n",
    ")\n",
    "sgd_trainer = Trainer(sgd_config)\n",
    "sgd_trainer.run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adamw_config = replace(\n",
    "    optimizer_config,\n",
    "    config_name=\"AdamW\",\n",
    "    optimizer=\"adamw\",\n",
    "    lr=3e-4,\n",
    ")\n",
    "adamw_trainer = Trainer(adamw_config)\n",
    "adamw_trainer.run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Layer Normalization and Dropout\n",
    "In this experiment, we will investigate the effects of Layer Normalization and Dropout on the performance of a neural network. We will train the model under four different configurations: \n",
    "1. Without Layer Normalization and Dropout\n",
    "2. With only Layer Normalization\n",
    "3. With only Dropout\n",
    "4. With both Layer Normalization and Dropout. \n",
    "\n",
    "What do you observe when analyzing the training and validation curves? Which configuration performs best, and what do you conclude about the benefits of using DropOut and/or LayerNorm?\n",
    "\n",
    "You can also play around and try to get better results - try different optimizers and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layernorm_dropout_config = ExperimentConfig(\n",
    "    exp_name=\"LayernormDropout\",\n",
    "    config_name=None,\n",
    ")\n",
    "\n",
    "base_config = replace(\n",
    "    layernorm_dropout_config,\n",
    "    config_name=\"no_ln_no_dropout\",\n",
    "    use_layernorm=False,\n",
    "    dropout_rate=0,\n",
    ")\n",
    "\n",
    "pprint(base_config)\n",
    "base_trainer = Trainer(base_config)\n",
    "base_trainer.run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_config = replace(\n",
    "    layernorm_dropout_config,\n",
    "    config_name=\"layernorm_only\",\n",
    "    use_layernorm=True,\n",
    "    dropout_rate=0,\n",
    ")\n",
    "ln_config_trainer = Trainer(ln_config)\n",
    "ln_config_trainer.run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_config = replace(\n",
    "    layernorm_dropout_config,\n",
    "    config_name=\"dropout_only\",\n",
    "    use_layernorm=False,\n",
    "    dropout_rate=0.15,\n",
    ")\n",
    "dropout_trainer = Trainer(dropout_config)\n",
    "dropout_trainer.run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_dropout_config = replace(\n",
    "    layernorm_dropout_config,\n",
    "    config_name=\"layernorm_and_dropout\",\n",
    "    use_layernorm=True,\n",
    "    dropout_rate=0.15,\n",
    ")\n",
    "ln_dropout_trainer = Trainer(ln_dropout_config)\n",
    "ln_dropout_trainer.run_experiment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
